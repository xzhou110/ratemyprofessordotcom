{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.core.display import display, HTML\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "import diagnostic_plots\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to pick for efficiency\n",
    "\n",
    "\n",
    "\n",
    "filename_berkely = '/Users/xzhou/github/project_files/project_luther/professor_data_berkely_add_features.pkl' #3986 records\n",
    "filename_nyu = '/Users/xzhou/github/project_files/project_luther/professor_data_nyu_add_features.pkl' #5607 records\n",
    "filename_uf = '/Users/xzhou/github/project_files/project_luther/professor_data_uf_add_features.pkl' #5307 records\n",
    "\n",
    "df_berkely=pd.read_pickle(filename_berkely)\n",
    "df_nyu=pd.read_pickle(filename_nyu)\n",
    "df_uf=pd.read_pickle(filename_uf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3986, 30)\n",
      "(5607, 30)\n",
      "(5306, 30)\n"
     ]
    }
   ],
   "source": [
    "print(df_berkely.shape)\n",
    "print(df_nyu.shape)\n",
    "print(df_uf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14899, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define key for each section. This would be used for future references. \n",
    "dfs=pd.concat([df_berkely, df_nyu, df_uf],keys=['berkely', 'nyu', 'uf'])\n",
    "dfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Level of difficulty</th>\n",
       "      <th>Total reviews</th>\n",
       "      <th>Student size</th>\n",
       "      <th>Type_public</th>\n",
       "      <th>Type_private</th>\n",
       "      <th>Region_west</th>\n",
       "      <th>Region_east</th>\n",
       "      <th>Region_south</th>\n",
       "      <th>...</th>\n",
       "      <th>Hilarious</th>\n",
       "      <th>Inspirational</th>\n",
       "      <th>Lecture heavy</th>\n",
       "      <th>Lots of homework</th>\n",
       "      <th>Participation matters</th>\n",
       "      <th>Respected</th>\n",
       "      <th>Skip class? you won't pass.</th>\n",
       "      <th>So many papers</th>\n",
       "      <th>Test heavy</th>\n",
       "      <th>Tough grader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">uf</th>\n",
       "      <th>5301</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>52367</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5302</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>52367</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>Tina D'Allesandro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>52367</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5304</th>\n",
       "      <td>Kay Leary</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>52367</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5305</th>\n",
       "      <td>Lisa Domenico</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>52367</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name  Rating  Level of difficulty  Total reviews  \\\n",
       "uf 5301                        0.0                  2.5              0   \n",
       "   5302                        0.0                  2.5              0   \n",
       "   5303  Tina D'Allesandro     4.0                  3.0              1   \n",
       "   5304          Kay Leary     5.0                  3.0              1   \n",
       "   5305      Lisa Domenico     4.0                  5.0              1   \n",
       "\n",
       "         Student size  Type_public  Type_private  Region_west  Region_east  \\\n",
       "uf 5301         52367            1             0            0            0   \n",
       "   5302         52367            1             0            0            0   \n",
       "   5303         52367            1             0            0            0   \n",
       "   5304         52367            1             0            0            0   \n",
       "   5305         52367            1             0            0            0   \n",
       "\n",
       "         Region_south      ...       Hilarious  Inspirational  Lecture heavy  \\\n",
       "uf 5301             1      ...             0.0            0.0            0.0   \n",
       "   5302             1      ...             0.0            0.0            0.0   \n",
       "   5303             1      ...             0.0            0.0            0.0   \n",
       "   5304             1      ...             0.0            0.0            0.0   \n",
       "   5305             1      ...             0.0            0.0            0.0   \n",
       "\n",
       "         Lots of homework  Participation matters  Respected  \\\n",
       "uf 5301               0.0                   0.00        0.0   \n",
       "   5302               0.0                   0.00        0.0   \n",
       "   5303               0.0                   0.33        0.0   \n",
       "   5304               0.0                   0.00        0.0   \n",
       "   5305               0.0                   0.33        0.0   \n",
       "\n",
       "         Skip class? you won't pass.  So many papers  Test heavy  Tough grader  \n",
       "uf 5301                          0.0             0.0         0.0           0.0  \n",
       "   5302                          0.0             0.0         0.0           0.0  \n",
       "   5303                          0.0             0.0         0.0           0.0  \n",
       "   5304                          0.0             0.0         0.0           0.0  \n",
       "   5305                          0.0             0.0         0.0           0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noted records with no professor name\n",
    "dfs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Level of difficulty</th>\n",
       "      <th>Total reviews</th>\n",
       "      <th>Student size</th>\n",
       "      <th>Type_public</th>\n",
       "      <th>Type_private</th>\n",
       "      <th>Region_west</th>\n",
       "      <th>Region_east</th>\n",
       "      <th>Region_south</th>\n",
       "      <th>...</th>\n",
       "      <th>Hilarious</th>\n",
       "      <th>Inspirational</th>\n",
       "      <th>Lecture heavy</th>\n",
       "      <th>Lots of homework</th>\n",
       "      <th>Participation matters</th>\n",
       "      <th>Respected</th>\n",
       "      <th>Skip class? you won't pass.</th>\n",
       "      <th>So many papers</th>\n",
       "      <th>Test heavy</th>\n",
       "      <th>Tough grader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">berkely</th>\n",
       "      <th>244</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Rating  Level of difficulty  Total reviews  Student size  \\\n",
       "berkely 244          0.0                  2.5              0         41910   \n",
       "        274          0.0                  2.5              0         41910   \n",
       "        376          0.0                  2.5              0         41910   \n",
       "        416          0.0                  2.5              0         41910   \n",
       "        478          0.0                  2.5              0         41910   \n",
       "\n",
       "             Type_public  Type_private  Region_west  Region_east  \\\n",
       "berkely 244            1             0            1            0   \n",
       "        274            1             0            1            0   \n",
       "        376            1             0            1            0   \n",
       "        416            1             0            1            0   \n",
       "        478            1             0            1            0   \n",
       "\n",
       "             Region_south      ...       Hilarious  Inspirational  \\\n",
       "berkely 244             0      ...             0.0            0.0   \n",
       "        274             0      ...             0.0            0.0   \n",
       "        376             0      ...             0.0            0.0   \n",
       "        416             0      ...             0.0            0.0   \n",
       "        478             0      ...             0.0            0.0   \n",
       "\n",
       "             Lecture heavy  Lots of homework  Participation matters  \\\n",
       "berkely 244            0.0               0.0                    0.0   \n",
       "        274            0.0               0.0                    0.0   \n",
       "        376            0.0               0.0                    0.0   \n",
       "        416            0.0               0.0                    0.0   \n",
       "        478            0.0               0.0                    0.0   \n",
       "\n",
       "             Respected  Skip class? you won't pass.  So many papers  \\\n",
       "berkely 244        0.0                          0.0             0.0   \n",
       "        274        0.0                          0.0             0.0   \n",
       "        376        0.0                          0.0             0.0   \n",
       "        416        0.0                          0.0             0.0   \n",
       "        478        0.0                          0.0             0.0   \n",
       "\n",
       "             Test heavy  Tough grader  \n",
       "berkely 244         0.0           0.0  \n",
       "        274         0.0           0.0  \n",
       "        376         0.0           0.0  \n",
       "        416         0.0           0.0  \n",
       "        478         0.0           0.0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[dfs.Name==\"\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13238, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove records that don't have professor names\n",
    "dfs_smaller=dfs[dfs.Name!=\"\"]\n",
    "dfs_smaller.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_smaller=dfs_smaller.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'Rating', 'Level of difficulty', 'Total reviews',\n",
       "       'Student size', 'Type_public', 'Type_private', 'Region_west',\n",
       "       'Region_east', 'Region_south', 'Accessible outside class',\n",
       "       'Amazing lectures', 'Beware of pop quizzes', 'Caring',\n",
       "       'Clear grading criteria', 'Extra credit', 'Get ready to read',\n",
       "       'Gives good feedback', 'Graded by few things', 'Group projects',\n",
       "       'Hilarious', 'Inspirational', 'Lecture heavy', 'Lots of homework',\n",
       "       'Participation matters', 'Respected', 'Skip class? you won't pass.',\n",
       "       'So many papers', 'Test heavy', 'Tough grader'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_smaller.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Level of difficulty</th>\n",
       "      <th>Total reviews</th>\n",
       "      <th>Student size</th>\n",
       "      <th>Type_public</th>\n",
       "      <th>Type_private</th>\n",
       "      <th>Region_west</th>\n",
       "      <th>Region_east</th>\n",
       "      <th>Region_south</th>\n",
       "      <th>...</th>\n",
       "      <th>Hilarious</th>\n",
       "      <th>Inspirational</th>\n",
       "      <th>Lecture heavy</th>\n",
       "      <th>Lots of homework</th>\n",
       "      <th>Participation matters</th>\n",
       "      <th>Respected</th>\n",
       "      <th>Skip class? you won't pass.</th>\n",
       "      <th>So many papers</th>\n",
       "      <th>Test heavy</th>\n",
       "      <th>Tough grader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">berkely</th>\n",
       "      <th>1</th>\n",
       "      <td>Chris Dolder</td>\n",
       "      <td>4.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>9</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob Calonico</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>29</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mary Kelsey</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>63</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>John Searle</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>47</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>William Hanks</td>\n",
       "      <td>4.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>25</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name  Rating  Level of difficulty  Total reviews  \\\n",
       "berkely 1   Chris Dolder     4.8                  3.9              9   \n",
       "        2   Bob Calonico     4.7                  1.6             29   \n",
       "        4    Mary Kelsey     4.6                  1.6             63   \n",
       "        5    John Searle     3.6                  3.1             47   \n",
       "        8  William Hanks     4.4                  2.2             25   \n",
       "\n",
       "           Student size  Type_public  Type_private  Region_west  Region_east  \\\n",
       "berkely 1         41910            1             0            1            0   \n",
       "        2         41910            1             0            1            0   \n",
       "        4         41910            1             0            1            0   \n",
       "        5         41910            1             0            1            0   \n",
       "        8         41910            1             0            1            0   \n",
       "\n",
       "           Region_south      ...       Hilarious  Inspirational  \\\n",
       "berkely 1             0      ...            0.00           0.50   \n",
       "        2             0      ...            0.26           0.32   \n",
       "        4             0      ...            0.00           0.08   \n",
       "        5             0      ...            0.21           0.07   \n",
       "        8             0      ...            0.00           0.33   \n",
       "\n",
       "           Lecture heavy  Lots of homework  Participation matters  Respected  \\\n",
       "berkely 1           0.00               0.0                   0.00       0.00   \n",
       "        2           0.00               0.0                   0.00       0.11   \n",
       "        4           0.04               0.0                   0.08       0.12   \n",
       "        5           0.00               0.0                   0.00       0.21   \n",
       "        8           0.00               0.0                   0.00       0.00   \n",
       "\n",
       "           Skip class? you won't pass.  So many papers  Test heavy  \\\n",
       "berkely 1                         0.00             0.0         0.0   \n",
       "        2                         0.00             0.0         0.0   \n",
       "        4                         0.04             0.0         0.0   \n",
       "        5                         0.00             0.0         0.0   \n",
       "        8                         0.33             0.0         0.0   \n",
       "\n",
       "           Tough grader  \n",
       "berkely 1          0.00  \n",
       "        2          0.00  \n",
       "        4          0.04  \n",
       "        5          0.07  \n",
       "        8          0.00  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear out records without tags\n",
    "dfs_smaller.insert(5,'Total score for Tags', dfs_smaller.sum(axis=1)-dfs_smaller['Rating']-dfs_smaller['Level of difficulty']-dfs_smaller['Total reviews']-dfs_smaller['Student size']-dfs_smaller['Type_public']-dfs_smaller['Type_private']-dfs_smaller['Region_west']-dfs_smaller['Region_east']-dfs_smaller['Region_south'])\n",
    "dfs_clean=dfs_smaller[dfs_smaller['Total score for Tags']>0.9]\n",
    "dfs_clean=dfs_clean.drop(['Total score for Tags'], 1)\n",
    "dfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on professor with meaningful number of reviews\n",
    "\n",
    "dfs_clean=dfs_clean[dfs_clean['Total reviews']>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3854, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Name', 'Rating', 'Level of difficulty', 'Total reviews',\n",
       "       'Student size', 'Type_public', 'Type_private', 'Region_west',\n",
       "       'Region_east', 'Region_south', 'Accessible outside class',\n",
       "       'Amazing lectures', 'Beware of pop quizzes', 'Caring',\n",
       "       'Clear grading criteria', 'Extra credit', 'Get ready to read',\n",
       "       'Gives good feedback', 'Graded by few things', 'Group projects',\n",
       "       'Hilarious', 'Inspirational', 'Lecture heavy', 'Lots of homework',\n",
       "       'Participation matters', 'Respected', 'Skip class? you won't pass.',\n",
       "       'So many papers', 'Test heavy', 'Tough grader'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Level of difficulty</th>\n",
       "      <th>Total reviews</th>\n",
       "      <th>Student size</th>\n",
       "      <th>Type_public</th>\n",
       "      <th>Type_private</th>\n",
       "      <th>Region_west</th>\n",
       "      <th>Region_east</th>\n",
       "      <th>Region_south</th>\n",
       "      <th>Accessible outside class</th>\n",
       "      <th>...</th>\n",
       "      <th>Hilarious</th>\n",
       "      <th>Inspirational</th>\n",
       "      <th>Lecture heavy</th>\n",
       "      <th>Lots of homework</th>\n",
       "      <th>Participation matters</th>\n",
       "      <th>Respected</th>\n",
       "      <th>Skip class? you won't pass.</th>\n",
       "      <th>So many papers</th>\n",
       "      <th>Test heavy</th>\n",
       "      <th>Tough grader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "      <td>3854.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.799611</td>\n",
       "      <td>3.014504</td>\n",
       "      <td>20.942916</td>\n",
       "      <td>52341.371562</td>\n",
       "      <td>0.581474</td>\n",
       "      <td>0.418526</td>\n",
       "      <td>0.270368</td>\n",
       "      <td>0.418526</td>\n",
       "      <td>0.311105</td>\n",
       "      <td>0.035594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047665</td>\n",
       "      <td>0.064440</td>\n",
       "      <td>0.051998</td>\n",
       "      <td>0.046194</td>\n",
       "      <td>0.062429</td>\n",
       "      <td>0.081995</td>\n",
       "      <td>0.060812</td>\n",
       "      <td>0.013140</td>\n",
       "      <td>0.017896</td>\n",
       "      <td>0.089398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.834313</td>\n",
       "      <td>0.720282</td>\n",
       "      <td>29.013844</td>\n",
       "      <td>6952.004682</td>\n",
       "      <td>0.493381</td>\n",
       "      <td>0.493381</td>\n",
       "      <td>0.444208</td>\n",
       "      <td>0.493381</td>\n",
       "      <td>0.463006</td>\n",
       "      <td>0.071648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096279</td>\n",
       "      <td>0.104846</td>\n",
       "      <td>0.106302</td>\n",
       "      <td>0.092767</td>\n",
       "      <td>0.106023</td>\n",
       "      <td>0.112470</td>\n",
       "      <td>0.103218</td>\n",
       "      <td>0.050819</td>\n",
       "      <td>0.053041</td>\n",
       "      <td>0.141815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>41910.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.300000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>41910.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>52367.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>59061.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>507.000000</td>\n",
       "      <td>59061.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Rating  Level of difficulty  Total reviews  Student size  \\\n",
       "count  3854.000000          3854.000000    3854.000000   3854.000000   \n",
       "mean      3.799611             3.014504      20.942916  52341.371562   \n",
       "std       0.834313             0.720282      29.013844   6952.004682   \n",
       "min       1.000000             1.000000       5.000000  41910.000000   \n",
       "25%       3.300000             2.500000       8.000000  41910.000000   \n",
       "50%       4.000000             3.000000      13.000000  52367.000000   \n",
       "75%       4.500000             3.500000      23.000000  59061.000000   \n",
       "max       5.000000             5.000000     507.000000  59061.000000   \n",
       "\n",
       "       Type_public  Type_private  Region_west  Region_east  Region_south  \\\n",
       "count  3854.000000   3854.000000  3854.000000  3854.000000   3854.000000   \n",
       "mean      0.581474      0.418526     0.270368     0.418526      0.311105   \n",
       "std       0.493381      0.493381     0.444208     0.493381      0.463006   \n",
       "min       0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "25%       0.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "50%       1.000000      0.000000     0.000000     0.000000      0.000000   \n",
       "75%       1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "max       1.000000      1.000000     1.000000     1.000000      1.000000   \n",
       "\n",
       "       Accessible outside class      ...         Hilarious  Inspirational  \\\n",
       "count               3854.000000      ...       3854.000000    3854.000000   \n",
       "mean                   0.035594      ...          0.047665       0.064440   \n",
       "std                    0.071648      ...          0.096279       0.104846   \n",
       "min                    0.000000      ...          0.000000       0.000000   \n",
       "25%                    0.000000      ...          0.000000       0.000000   \n",
       "50%                    0.000000      ...          0.000000       0.000000   \n",
       "75%                    0.050000      ...          0.060000       0.100000   \n",
       "max                    1.000000      ...          1.000000       1.000000   \n",
       "\n",
       "       Lecture heavy  Lots of homework  Participation matters    Respected  \\\n",
       "count    3854.000000       3854.000000            3854.000000  3854.000000   \n",
       "mean        0.051998          0.046194               0.062429     0.081995   \n",
       "std         0.106302          0.092767               0.106023     0.112470   \n",
       "min         0.000000          0.000000               0.000000     0.000000   \n",
       "25%         0.000000          0.000000               0.000000     0.000000   \n",
       "50%         0.000000          0.000000               0.000000     0.030000   \n",
       "75%         0.070000          0.060000               0.100000     0.137500   \n",
       "max         1.000000          1.000000               1.000000     1.000000   \n",
       "\n",
       "       Skip class? you won't pass.  So many papers   Test heavy  Tough grader  \n",
       "count                  3854.000000     3854.000000  3854.000000   3854.000000  \n",
       "mean                      0.060812        0.013140     0.017896      0.089398  \n",
       "std                       0.103218        0.050819     0.053041      0.141815  \n",
       "min                       0.000000        0.000000     0.000000      0.000000  \n",
       "25%                       0.000000        0.000000     0.000000      0.000000  \n",
       "50%                       0.000000        0.000000     0.000000      0.000000  \n",
       "75%                       0.090000        0.000000     0.000000      0.140000  \n",
       "max                       1.000000        0.500000     0.500000      1.000000  \n",
       "\n",
       "[8 rows x 29 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c27ef06a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs_clean['Rating'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Level of difficulty</th>\n",
       "      <th>Total reviews</th>\n",
       "      <th>Student size</th>\n",
       "      <th>Type_public</th>\n",
       "      <th>Type_private</th>\n",
       "      <th>Region_west</th>\n",
       "      <th>Region_east</th>\n",
       "      <th>Region_south</th>\n",
       "      <th>Accessible outside class</th>\n",
       "      <th>...</th>\n",
       "      <th>Hilarious</th>\n",
       "      <th>Inspirational</th>\n",
       "      <th>Lecture heavy</th>\n",
       "      <th>Lots of homework</th>\n",
       "      <th>Participation matters</th>\n",
       "      <th>Respected</th>\n",
       "      <th>Skip class? you won't pass.</th>\n",
       "      <th>So many papers</th>\n",
       "      <th>Test heavy</th>\n",
       "      <th>Tough grader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">berkely</th>\n",
       "      <th>1</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.9</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3.367296</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.143135</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.850148</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>41910</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Rating  Level of difficulty  Total reviews  Student size  \\\n",
       "berkely 1     4.8                  3.9       2.197225         41910   \n",
       "        2     4.7                  1.6       3.367296         41910   \n",
       "        4     4.6                  1.6       4.143135         41910   \n",
       "        5     3.6                  3.1       3.850148         41910   \n",
       "        8     4.4                  2.2       3.218876         41910   \n",
       "\n",
       "           Type_public  Type_private  Region_west  Region_east  Region_south  \\\n",
       "berkely 1            1             0            1            0             0   \n",
       "        2            1             0            1            0             0   \n",
       "        4            1             0            1            0             0   \n",
       "        5            1             0            1            0             0   \n",
       "        8            1             0            1            0             0   \n",
       "\n",
       "           Accessible outside class      ...       Hilarious  Inspirational  \\\n",
       "berkely 1                      0.00      ...            0.00           0.50   \n",
       "        2                      0.05      ...            0.26           0.32   \n",
       "        4                      0.04      ...            0.00           0.08   \n",
       "        5                      0.00      ...            0.21           0.07   \n",
       "        8                      0.00      ...            0.00           0.33   \n",
       "\n",
       "           Lecture heavy  Lots of homework  Participation matters  Respected  \\\n",
       "berkely 1           0.00               0.0                   0.00       0.00   \n",
       "        2           0.00               0.0                   0.00       0.11   \n",
       "        4           0.04               0.0                   0.08       0.12   \n",
       "        5           0.00               0.0                   0.00       0.21   \n",
       "        8           0.00               0.0                   0.00       0.00   \n",
       "\n",
       "           Skip class? you won't pass.  So many papers  Test heavy  \\\n",
       "berkely 1                         0.00             0.0         0.0   \n",
       "        2                         0.00             0.0         0.0   \n",
       "        4                         0.04             0.0         0.0   \n",
       "        5                         0.00             0.0         0.0   \n",
       "        8                         0.33             0.0         0.0   \n",
       "\n",
       "           Tough grader  \n",
       "berkely 1          0.00  \n",
       "        2          0.00  \n",
       "        4          0.04  \n",
       "        5          0.07  \n",
       "        8          0.00  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform transformation of Total reviews and drop unnecessary fields\n",
    "dfs_clean['Total reviews']=np.log(dfs_clean['Total reviews'])\n",
    "dfs_clean=dfs_clean.drop('Name', axis=1)\n",
    "dfs_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(dfs_clean,  size = 2, aspect=1.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform training and test sets split:\n",
    "\n",
    "def tt_split(df, y_column, test_size):\n",
    "    X=df.drop(y_column, 1)\n",
    "    y=df[y_column]\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Rating</td>      <th>  R-squared:         </th> <td>   0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   29.87</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 20 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>2.01e-92</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:55:57</td>     <th>  Log-Likelihood:    </th> <td> -721.05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   770</td>      <th>  AIC:               </th> <td>   1492.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   745</td>      <th>  BIC:               </th> <td>   1608.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    24</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                       <td>    0.1667</td> <td>    0.060</td> <td>    2.790</td> <td> 0.005</td> <td>    0.049</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Level of difficulty</th>         <td>   -0.2987</td> <td>    0.037</td> <td>   -8.135</td> <td> 0.000</td> <td>   -0.371</td> <td>   -0.227</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Total reviews</th>               <td>   -0.0314</td> <td>    0.031</td> <td>   -1.008</td> <td> 0.314</td> <td>   -0.093</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Student size</th>                <td> 9.452e-05</td> <td> 3.27e-05</td> <td>    2.892</td> <td> 0.004</td> <td> 3.03e-05</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Type_public</th>                 <td>    0.4307</td> <td>    0.146</td> <td>    2.956</td> <td> 0.003</td> <td>    0.145</td> <td>    0.717</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Type_private</th>                <td>   -0.2640</td> <td>    0.086</td> <td>   -3.056</td> <td> 0.002</td> <td>   -0.434</td> <td>   -0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Region_west</th>                 <td>    0.6660</td> <td>    0.245</td> <td>    2.721</td> <td> 0.007</td> <td>    0.185</td> <td>    1.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Region_east</th>                 <td>   -0.2640</td> <td>    0.086</td> <td>   -3.056</td> <td> 0.002</td> <td>   -0.434</td> <td>   -0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Region_south</th>                <td>   -0.2352</td> <td>    0.102</td> <td>   -2.306</td> <td> 0.021</td> <td>   -0.435</td> <td>   -0.035</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Accessible outside class</th>    <td>    0.2782</td> <td>    1.850</td> <td>    0.150</td> <td> 0.881</td> <td>   -3.354</td> <td>    3.910</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amazing lectures</th>            <td>    0.7110</td> <td>    1.835</td> <td>    0.388</td> <td> 0.698</td> <td>   -2.890</td> <td>    4.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Beware of pop quizzes</th>       <td>   -0.0913</td> <td>    1.918</td> <td>   -0.048</td> <td> 0.962</td> <td>   -3.858</td> <td>    3.675</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Caring</th>                      <td>    0.6095</td> <td>    1.834</td> <td>    0.332</td> <td> 0.740</td> <td>   -2.992</td> <td>    4.211</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Clear grading criteria</th>      <td>   -0.8394</td> <td>    1.839</td> <td>   -0.457</td> <td> 0.648</td> <td>   -4.449</td> <td>    2.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Extra credit</th>                <td>   -0.3720</td> <td>    1.858</td> <td>   -0.200</td> <td> 0.841</td> <td>   -4.019</td> <td>    3.275</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Get ready to read</th>           <td>   -0.7956</td> <td>    1.833</td> <td>   -0.434</td> <td> 0.664</td> <td>   -4.394</td> <td>    2.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Gives good feedback</th>         <td>    0.0265</td> <td>    1.829</td> <td>    0.015</td> <td> 0.988</td> <td>   -3.564</td> <td>    3.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Graded by few things</th>        <td>   -2.0161</td> <td>    1.863</td> <td>   -1.082</td> <td> 0.279</td> <td>   -5.673</td> <td>    1.641</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group projects</th>              <td>   -2.2659</td> <td>    1.870</td> <td>   -1.211</td> <td> 0.226</td> <td>   -5.938</td> <td>    1.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Hilarious</th>                   <td>    0.0618</td> <td>    1.834</td> <td>    0.034</td> <td> 0.973</td> <td>   -3.538</td> <td>    3.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Inspirational</th>               <td>   -0.1219</td> <td>    1.831</td> <td>   -0.067</td> <td> 0.947</td> <td>   -3.717</td> <td>    3.473</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lecture heavy</th>               <td>   -1.5904</td> <td>    1.837</td> <td>   -0.866</td> <td> 0.387</td> <td>   -5.196</td> <td>    2.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lots of homework</th>            <td>   -1.0009</td> <td>    1.828</td> <td>   -0.547</td> <td> 0.584</td> <td>   -4.590</td> <td>    2.588</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Participation matters</th>       <td>   -0.5623</td> <td>    1.834</td> <td>   -0.307</td> <td> 0.759</td> <td>   -4.162</td> <td>    3.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Respected</th>                   <td>    0.2398</td> <td>    1.831</td> <td>    0.131</td> <td> 0.896</td> <td>   -3.355</td> <td>    3.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skip class? you won't pass.</th> <td>   -0.4350</td> <td>    1.832</td> <td>   -0.237</td> <td> 0.812</td> <td>   -4.031</td> <td>    3.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>So many papers</th>              <td>   -1.4836</td> <td>    1.867</td> <td>   -0.795</td> <td> 0.427</td> <td>   -5.148</td> <td>    2.181</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Test heavy</th>                  <td>   -2.6990</td> <td>    1.873</td> <td>   -1.441</td> <td> 0.150</td> <td>   -6.376</td> <td>    0.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Tough grader</th>                <td>   -1.5367</td> <td>    1.828</td> <td>   -0.841</td> <td> 0.401</td> <td>   -5.125</td> <td>    2.052</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>35.093</td> <th>  Durbin-Watson:     </th> <td>   2.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  39.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.502</td> <th>  Prob(JB):          </th> <td>2.62e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.473</td> <th>  Cond. No.          </th> <td>3.11e+17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.22e-23. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 Rating   R-squared:                       0.490\n",
       "Model:                            OLS   Adj. R-squared:                  0.474\n",
       "Method:                 Least Squares   F-statistic:                     29.87\n",
       "Date:                Fri, 20 Jul 2018   Prob (F-statistic):           2.01e-92\n",
       "Time:                        01:55:57   Log-Likelihood:                -721.05\n",
       "No. Observations:                 770   AIC:                             1492.\n",
       "Df Residuals:                     745   BIC:                             1608.\n",
       "Df Model:                          24                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================================\n",
       "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------\n",
       "const                           0.1667      0.060      2.790      0.005       0.049       0.284\n",
       "Level of difficulty            -0.2987      0.037     -8.135      0.000      -0.371      -0.227\n",
       "Total reviews                  -0.0314      0.031     -1.008      0.314      -0.093       0.030\n",
       "Student size                 9.452e-05   3.27e-05      2.892      0.004    3.03e-05       0.000\n",
       "Type_public                     0.4307      0.146      2.956      0.003       0.145       0.717\n",
       "Type_private                   -0.2640      0.086     -3.056      0.002      -0.434      -0.094\n",
       "Region_west                     0.6660      0.245      2.721      0.007       0.185       1.147\n",
       "Region_east                    -0.2640      0.086     -3.056      0.002      -0.434      -0.094\n",
       "Region_south                   -0.2352      0.102     -2.306      0.021      -0.435      -0.035\n",
       "Accessible outside class        0.2782      1.850      0.150      0.881      -3.354       3.910\n",
       "Amazing lectures                0.7110      1.835      0.388      0.698      -2.890       4.312\n",
       "Beware of pop quizzes          -0.0913      1.918     -0.048      0.962      -3.858       3.675\n",
       "Caring                          0.6095      1.834      0.332      0.740      -2.992       4.211\n",
       "Clear grading criteria         -0.8394      1.839     -0.457      0.648      -4.449       2.770\n",
       "Extra credit                   -0.3720      1.858     -0.200      0.841      -4.019       3.275\n",
       "Get ready to read              -0.7956      1.833     -0.434      0.664      -4.394       2.803\n",
       "Gives good feedback             0.0265      1.829      0.015      0.988      -3.564       3.617\n",
       "Graded by few things           -2.0161      1.863     -1.082      0.279      -5.673       1.641\n",
       "Group projects                 -2.2659      1.870     -1.211      0.226      -5.938       1.406\n",
       "Hilarious                       0.0618      1.834      0.034      0.973      -3.538       3.662\n",
       "Inspirational                  -0.1219      1.831     -0.067      0.947      -3.717       3.473\n",
       "Lecture heavy                  -1.5904      1.837     -0.866      0.387      -5.196       2.015\n",
       "Lots of homework               -1.0009      1.828     -0.547      0.584      -4.590       2.588\n",
       "Participation matters          -0.5623      1.834     -0.307      0.759      -4.162       3.037\n",
       "Respected                       0.2398      1.831      0.131      0.896      -3.355       3.834\n",
       "Skip class? you won't pass.    -0.4350      1.832     -0.237      0.812      -4.031       3.161\n",
       "So many papers                 -1.4836      1.867     -0.795      0.427      -5.148       2.181\n",
       "Test heavy                     -2.6990      1.873     -1.441      0.150      -6.376       0.978\n",
       "Tough grader                   -1.5367      1.828     -0.841      0.401      -5.125       2.052\n",
       "==============================================================================\n",
       "Omnibus:                       35.093   Durbin-Watson:                   2.050\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               39.523\n",
       "Skew:                          -0.502   Prob(JB):                     2.62e-09\n",
       "Kurtosis:                       3.473   Cond. No.                     3.11e+17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.22e-23. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prior to normalize\n",
    "X_train, X_test, y_train, y_test=tt_split(dfs_clean, 'Rating', 0.8)\n",
    "\n",
    "model1=sm.OLS(y_train, sm.add_constant(X_train))\n",
    "fit1=model1.fit()\n",
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Rating</td>      <th>  R-squared:         </th> <td>   0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   29.87</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 20 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>2.01e-92</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:55:58</td>     <th>  Log-Likelihood:    </th> <td> -721.05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   770</td>      <th>  AIC:               </th> <td>   1492.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   745</td>      <th>  BIC:               </th> <td>   1608.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    24</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    3.7900</td> <td>    0.023</td> <td>  167.598</td> <td> 0.000</td> <td>    3.746</td> <td>    3.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.2175</td> <td>    0.027</td> <td>   -8.135</td> <td> 0.000</td> <td>   -0.270</td> <td>   -0.165</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.0234</td> <td>    0.023</td> <td>   -1.008</td> <td> 0.314</td> <td>   -0.069</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0055</td> <td>    0.008</td> <td>    0.715</td> <td> 0.475</td> <td>   -0.010</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0080</td> <td>    0.006</td> <td>    1.386</td> <td> 0.166</td> <td>   -0.003</td> <td>    0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0080</td> <td>    0.006</td> <td>   -1.386</td> <td> 0.166</td> <td>   -0.019</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -0.0140</td> <td>    0.011</td> <td>   -1.242</td> <td> 0.215</td> <td>   -0.036</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.0080</td> <td>    0.006</td> <td>   -1.386</td> <td> 0.166</td> <td>   -0.019</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.0219</td> <td>    0.013</td> <td>    1.687</td> <td> 0.092</td> <td>   -0.004</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.0183</td> <td>    0.122</td> <td>    0.150</td> <td> 0.881</td> <td>   -0.221</td> <td>    0.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>    0.0797</td> <td>    0.206</td> <td>    0.388</td> <td> 0.698</td> <td>   -0.324</td> <td>    0.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -0.0040</td> <td>    0.085</td> <td>   -0.048</td> <td> 0.962</td> <td>   -0.171</td> <td>    0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.0641</td> <td>    0.193</td> <td>    0.332</td> <td> 0.740</td> <td>   -0.314</td> <td>    0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.0785</td> <td>    0.172</td> <td>   -0.457</td> <td> 0.648</td> <td>   -0.416</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>   -0.0219</td> <td>    0.109</td> <td>   -0.200</td> <td> 0.841</td> <td>   -0.236</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>   -0.0987</td> <td>    0.227</td> <td>   -0.434</td> <td> 0.664</td> <td>   -0.545</td> <td>    0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.0031</td> <td>    0.212</td> <td>    0.015</td> <td> 0.988</td> <td>   -0.413</td> <td>    0.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>   -0.1472</td> <td>    0.136</td> <td>   -1.082</td> <td> 0.279</td> <td>   -0.414</td> <td>    0.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>   -0.1282</td> <td>    0.106</td> <td>   -1.211</td> <td> 0.226</td> <td>   -0.336</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0060</td> <td>    0.179</td> <td>    0.034</td> <td> 0.973</td> <td>   -0.346</td> <td>    0.358</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>   -0.0146</td> <td>    0.219</td> <td>   -0.067</td> <td> 0.947</td> <td>   -0.445</td> <td>    0.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>   -0.1889</td> <td>    0.218</td> <td>   -0.866</td> <td> 0.387</td> <td>   -0.617</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>   -0.0923</td> <td>    0.169</td> <td>   -0.547</td> <td> 0.584</td> <td>   -0.423</td> <td>    0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>   -0.0571</td> <td>    0.186</td> <td>   -0.307</td> <td> 0.759</td> <td>   -0.423</td> <td>    0.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.0272</td> <td>    0.208</td> <td>    0.131</td> <td> 0.896</td> <td>   -0.381</td> <td>    0.435</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>   -0.0422</td> <td>    0.178</td> <td>   -0.237</td> <td> 0.812</td> <td>   -0.391</td> <td>    0.307</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.0727</td> <td>    0.091</td> <td>   -0.795</td> <td> 0.427</td> <td>   -0.252</td> <td>    0.107</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.1301</td> <td>    0.090</td> <td>   -1.441</td> <td> 0.150</td> <td>   -0.307</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.2159</td> <td>    0.257</td> <td>   -0.841</td> <td> 0.401</td> <td>   -0.720</td> <td>    0.288</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>35.093</td> <th>  Durbin-Watson:     </th> <td>   2.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  39.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.502</td> <th>  Prob(JB):          </th> <td>2.62e-09</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.473</td> <th>  Cond. No.          </th> <td>1.61e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.32e-29. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 Rating   R-squared:                       0.490\n",
       "Model:                            OLS   Adj. R-squared:                  0.474\n",
       "Method:                 Least Squares   F-statistic:                     29.87\n",
       "Date:                Fri, 20 Jul 2018   Prob (F-statistic):           2.01e-92\n",
       "Time:                        01:55:58   Log-Likelihood:                -721.05\n",
       "No. Observations:                 770   AIC:                             1492.\n",
       "Df Residuals:                     745   BIC:                             1608.\n",
       "Df Model:                          24                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          3.7900      0.023    167.598      0.000       3.746       3.834\n",
       "x1            -0.2175      0.027     -8.135      0.000      -0.270      -0.165\n",
       "x2            -0.0234      0.023     -1.008      0.314      -0.069       0.022\n",
       "x3             0.0055      0.008      0.715      0.475      -0.010       0.021\n",
       "x4             0.0080      0.006      1.386      0.166      -0.003       0.019\n",
       "x5            -0.0080      0.006     -1.386      0.166      -0.019       0.003\n",
       "x6            -0.0140      0.011     -1.242      0.215      -0.036       0.008\n",
       "x7            -0.0080      0.006     -1.386      0.166      -0.019       0.003\n",
       "x8             0.0219      0.013      1.687      0.092      -0.004       0.047\n",
       "x9             0.0183      0.122      0.150      0.881      -0.221       0.258\n",
       "x10            0.0797      0.206      0.388      0.698      -0.324       0.483\n",
       "x11           -0.0040      0.085     -0.048      0.962      -0.171       0.163\n",
       "x12            0.0641      0.193      0.332      0.740      -0.314       0.443\n",
       "x13           -0.0785      0.172     -0.457      0.648      -0.416       0.259\n",
       "x14           -0.0219      0.109     -0.200      0.841      -0.236       0.192\n",
       "x15           -0.0987      0.227     -0.434      0.664      -0.545       0.348\n",
       "x16            0.0031      0.212      0.015      0.988      -0.413       0.419\n",
       "x17           -0.1472      0.136     -1.082      0.279      -0.414       0.120\n",
       "x18           -0.1282      0.106     -1.211      0.226      -0.336       0.080\n",
       "x19            0.0060      0.179      0.034      0.973      -0.346       0.358\n",
       "x20           -0.0146      0.219     -0.067      0.947      -0.445       0.415\n",
       "x21           -0.1889      0.218     -0.866      0.387      -0.617       0.239\n",
       "x22           -0.0923      0.169     -0.547      0.584      -0.423       0.239\n",
       "x23           -0.0571      0.186     -0.307      0.759      -0.423       0.309\n",
       "x24            0.0272      0.208      0.131      0.896      -0.381       0.435\n",
       "x25           -0.0422      0.178     -0.237      0.812      -0.391       0.307\n",
       "x26           -0.0727      0.091     -0.795      0.427      -0.252       0.107\n",
       "x27           -0.1301      0.090     -1.441      0.150      -0.307       0.047\n",
       "x28           -0.2159      0.257     -0.841      0.401      -0.720       0.288\n",
       "==============================================================================\n",
       "Omnibus:                       35.093   Durbin-Watson:                   2.050\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               39.523\n",
       "Skew:                          -0.502   Prob(JB):                     2.62e-09\n",
       "Kurtosis:                       3.473   Cond. No.                     1.61e+16\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.32e-29. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model: noted y is not normal distribution. Leave it untransformed in this scenario\n",
    "\n",
    "# Perform training and testing splits\n",
    "\n",
    "X_train, X_test, y_train, y_test=tt_split(dfs_clean, 'Rating', 0.8)\n",
    "\n",
    "# normalize training set\n",
    "\n",
    "ssX = StandardScaler()\n",
    "X_train_scaled = ssX.fit_transform(X_train)\n",
    "\n",
    "\n",
    "model1=sm.OLS(y_train, sm.add_constant(X_train_scaled))\n",
    "fit1=model1.fit()\n",
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Risidual Plot')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAHPCAYAAABHrYwpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsvX9wVcd5//9cCRARkvhhZNnogp35CGNE2gT/UmkSMMEuacBKHJcmJHWSxm5pkn4m6cTJ1824jOuPJ41rd9I0bZOY4k5+uK5LPokNYSbUpuRDkzCKTe0kRo4CbTBIECEMBgmMEbr3+4d8xNW5u3t29+zu2T33/ZrJxFzde86ePbvPvvfZZ58tlMvlMgEAAAAAAACY1GVdAAAAAAAAAHwGghkAAAAAAAABEMwAAAAAAAAIgGAGAAAAAABAAAQzAAAAAAAAAiCYAQAAAAAAEADBDAAAhlm0aBHdcsst9O53v5ve85730OrVq+m2226jn//850RE9Nhjj9HDDz8svMYf/dEf0YEDB6o+//73v0+33367dtl6enpo7dq1VZ/39/fT4sWL6d3vfvfE/7q7u+nb3/42ERF95zvfoQ0bNiRe/5577qEXXnhBu3wAAOAjU7IuAAAA5JGvf/3rNGfOnIl/b968me6//356/PHHaf369Ym/37Rpk83iMZk+fTo9+eSTE/8eHByktWvX0pve9Cbpa/z4xz+m973vfTaKBwAAmQEPMwAAWObChQt09OhRmjlzJhERffnLX6b77ruPiIj+5V/+hbq7u+m2226jD3zgAxNe5Xe84x0THukvfelLdNNNN9Hv/d7v0VNPPTVx3bvvvps2b97M/PeuXbvo/e9/P733ve+lG2+8kf72b/9WudxtbW10xRVX0MGDByd9/utf/5r+5E/+hG655RZau3Yt/dM//RMREX3xi1+kY8eO0V133UU//elPle8HAAC+Ag8zAABY4MMf/jAREZ08eZIaGhpo5cqV9Fd/9VeTvjM2Nkaf//zn6T/+4z/o0ksvpSeeeIL27t1LHR0dE995+umn6d///d/piSeeoOnTp9MnPvGJxHuXy2V65JFH6Atf+AJdeeWVNDg4SCtXrqQPfehDSs/w3HPP0aFDh+jNb34z7dmzZ+Lzu+66i1atWkV/+Id/SMPDw/TBD36QLr/8cvqzP/sz2rZtGz300EP0G7/xG0r3AgAAn4FgBgAAC0QhGfv27aM//uM/pq6uLrrkkksmfae+vp7e+c530vvf/3668cYb6W1vexutWLFi0nf27NlDN998MzU1NRER0W233Ubf/OY3hfcuFAr01a9+lX7wgx/Q9773Pfrv//5vKpfL9Oqrrwp/d+7cOXr3u99NRONifvbs2fTggw/S5ZdfPvGds2fP0n/913/RI488QkREzc3N9N73vpd2795Na9askascAAAIDAhmAACwyJIlS+jP//zP6e6776bFixdTsVic9PeHHnqIfvnLX9KPf/xjevjhh+nJJ5+kL33pS5O+Uy6XJ/67vr5+4r8LhcKkv42OjhLRuKi99dZb6aabbqLrrruObrvtNnr66acnfZdFPIaZRalUqrpOqVSiCxcuCH8HAAAhgxhmAACwzNq1a+k3f/M3q0IyTpw4QStWrKBZs2bRRz7yEfrUpz41EbccsXz5cvr+979Pp0+fplKpNEnQzp49eyIjxeDgIP3kJz8hIqKXXnqJRkZG6FOf+hS94x3voJ6eHjp//jyVSqXUz9LU1ERvfvOb6dFHHyUiouHhYXriiSfot3/7t4loXNBDPAMA8gY8zAAA4IC/+Iu/oO7ubvrP//zPic/mzJlDH/vYx+gjH/kITZ8+nerr6+n++++f9LsVK1ZQX18f3XbbbdTS0kJXX301nTx5koiIbr/9drrrrrto9erVVCwW6bd+67eIaDyt3Y033ki/+7u/S9OmTaOrrrqKOjo66KWXXqJp06alfpaHHnqI7rvvPvrOd75D58+fp1tuuYXe+973EhHRzTffTJ/5zGfo3nvvpbe97W2p7wUAAD5QKCet0QEAAAAAAFDDICQDAAAAAAAAARDMAAAAAAAACIBgBgAAAAAAQAAEMwAAAAAAAAIgmAEAAAAAABDgfVq5oaHhTO47e3YjnTx5NpN7hw7qTh/UXTpQf/qg7vRB3emDutMHdacPr+5aW5u5v4GHmcOUKfXJXwJMUHf6oO7SgfrTB3WnD+pOH9SdPqg7fXTqDoIZAAAAAAAAARDMAAAAAAAACIBgBgAAAAAAQAAEMwAAAAAAAAIgmAEAAAAAABAAwQwAAAAAAIAACGYAAAAAAAAEOD24ZHR0lD73uc/RwMAAnT9/nj72sY/RqlWrXBYBAAAAAAAAJZwK5q1bt9KsWbPowQcfpJMnT9Ktt94KwQwAAAAAALzGqWB+5zvfSatXr574d309TqkBAAAAAAB+UyiXy2XXNx0ZGaGPfexj9Pu///t0yy23CL974cIYjn8EAAAAAACZ4dTDTER09OhR+sQnPkEf+MAHEsUyEdHJk2cdlKqa1tZmGhoazuTeoYO60wd1lw7Unz6oO31Qd/qg7vRB3enDq7vW1mbub5wK5uPHj9NHP/pR2rhxIy1btszlrQEAAAAAANDCaVq5r371q3T69Gn6x3/8R7r99tvp9ttvp3PnzrksAgAAAAAAAEo49TDfc889dM8997i8JQAAAAAAAKlwHsMMANCnp3eQtu85SEeOn6V5cxtpzbIrqauzLetiAQAAALkGghmAQOjpHaSvbd038e/+oTMT/4ZoBgAAAOyBo7EBCITtew5yPn/JaTkAAACAWgOCGYBAOHKcnWLx6MtnHJcEAAAAqC0gmAEIhHlzG5mfX37JDMclAQAAAGoLCGYAAmHNsis5n1/htiAAAABAjYFNfwAEQrSxb/uel+joy2fo8ktm0JplV2DDHwAAAGAZCGYAAqKrsw0CGQAAAHAMQjIAAAAAAAAQAMEMAAAAAACAAAhmAAAAAAAABEAwAwAAAAAAIACCGQAAAAAAAAEQzAAAAAAAAAiAYAYAAAAAAEAABDMAAAAAAAACIJgBAAAAAAAQAMEMAAAAAACAAAhmAAAAAAAABEAwAwAAAAAAIACCGQAAAAAAAAEQzAAAAAAAAAiAYAYAAAAAAEAABDMAAAAAAAACIJgBAAAAAAAQAMEMAAAAAACAAAhmAAAAAAAABEAwAwAAAAAAIACCGQAAAAAAAAEQzAAAAAAAAAiAYAYAAAAAAEAABDMAAAAAAAACIJgBAAAAAAAQAMEMAAAAAACAAAhmAAAAAAAABEAwAwAAAAAAIACCGQAAAAAAAAEQzAAAAAAAAAiYknUBgF/09A7S9j0H6cjxszRvbiOtWXYldXW2ZV0sAAAAAIDMgGAGE/T0DtLXtu6b+Hf/0JmJf0M0AwAAAKBWgWAGE2zfc5Dz+UsQzAAAAMDrhLoaG2q5fQCCGUxw5PhZ5udHXz7juCQg78BoAwBCJdTV2FDL7QvY9AcmmDe3kfn55ZfMcFwSkGcio90/dIZK5fKE0e7pHcy6aAAAkIhoNdZnQi23L8DDXCPIePTWLLty0uzz4udXOColqAUQ+gMACJlQV2NDLbcvQDDXALLLMNF/b9/zEh19+QxdfskMWrPsCoiYQPE17AFGGwAQMvPmNlL/ULW98n01NtRy+wIEcw2g4tHr6mzzQlSBdPgcqwajDQAImVBXY0Mtty8ghrkGgEev9vA5Vm3Nsis5n8NoAwD8p6uzjTZ0L6FiaxPV1xWo2NpEG7qXZO6MSCLUcvsCPMw1ADx6tYfPkySE/gAAQifU1dhQy+0DEMw1AJZhag/fJ0kw2gAAAEICIRk1AJZhag+EPQAAAADmgIe5RoBHr7ZA2AMAAABgDghmAHIKJkkAAACAGRCSAQAAAAAAgAAIZgAAAAAAAARAMAMAAAAAACAAghkAAAAAAAABEMwAAAAAAAAIgGAGAAAAAABAAAQzAAAAAAAAAiCYAQAAAAAAEICDS0Bw9PQO0vY9B+nI8bM0b24jrVl2JQ7oAAAAAIA1IJhBUPT0DtLXtu6b+Hf/0JmJf0M0A6BG5eRzwWXNtPr6+ehHAADAAIIZEFE4Xtvtew5yPn/Jy/IC4CvxyefBo6cx+QQAAA6IYQYTA2f/0BkqlcsTXtue3sGsi1bFkeNnmZ8fffmM45IAEDaiyScAAIDJQDCDoAbOeXMbmZ9ffskMxyUBIGww+QQAAHkQkgGCGjjXLLty0jLyxc+vyKA0AITLvLmN1D9U3ccx+QSVhBKuB4BtIJhBUANnZKi373mJjr58hi6/ZAatWXYFDDgAimDyCZLAJmuzYPIRNhDMILiBs6uzDUYGgJTEJ5/z25AlA0wGm6zNgclH+EAwA3htAahRKiefra3NNDQ0nHGJgE+EFK7nO5h8hA8EMyAieG0BAABMJqRwPd/B5CN8kCUDAAAAAFWsWXYl53M/w/V8BhmewgceZgBAcGDzDAD2QbieOULbKwSqgWAGAASFaPPM2hXNWRULAO8RTTR5f0O4nhkw+QgfCGYAQFCINs+sXdHhtCwAhIJooklEyODgAEw+wgaCGQAQFNg8wwZhKkCE+ETXMvdvaEMAjAPBDAAICuzcrwY5Xv3A50mLaKJZZuvlmp+EAlAJBDMAICiweaYa5HjNHtuTlrRiXDzRLAcxCfV5QgLyDwQzAMAYLgY0bJ6pBmEq2WNz0mJCjCdNNH2fhGIVBWQNBDMAwAguBzRsnpkMwlSyx+akxYQYl5lo+jwJxSoKyBoIZgCAETCgZQfCVLLH5qTFlBgXTTR9n4RiFQVkDQQz8BbEq4UFBrTsQJhK9tictGAFAXUAsgeCGXgJ4tXCAwNatvjuIcw7NictJsV4qI4IrKKArMlEMP/0pz+lhx56iL75zW9mcXsQAFjeDw8MaKDWsTVpMSXGQ3ZEYBUFZI1zwbxp0ybaunUrveENb3B9axAQWN4PDwxoANjDhBgP3RGR11WUUL3+tYZzwbxgwQL68pe/TJ/97Gdd3xpokkVnxvJ+mOR1QAMgD8AR4R8he/1rDeeCefXq1dTf3y/9/dmzG2nKlHqLJeLT2tqcyX19Yvdz/czO3NIynZYvLXJ/l7bu1q++mh781l7G54ty/17y/ny2Qf3pE1Ld7X6un7bs3E+HBodpQVszrVu1UGiTUBaiBZc108Gjp6t+M7+tOdN3H1K7M82OZ57lfH6Y1q7oSPx9LdddWlTrzvtNfydPsmfEtmltbaahoeHU19Hxzvq0PPPYjl9wPu+jxcWZzL+ZqLvFxZm0oXtJ1fL+4uJMI+/FV0y1u1oF9adPSHUX98odPHqaHvzWXjp9+pxzWxlSWVZfP5+5z2D19fMze/dZtjsfxtpDv2Y/++HB4cR6CanP+gav7kQi2nvBHDI6Sy2+Lc9kuYSH5X0AAAufYnFDKovKPgMfxKRNfBlrEX4YDhDMFtExpD4ZXyJ0ZgCAf/gUixtaWWQcEb6ISZv4MtYiu1A4ZCKYi8Ui/du//VsWt3aKjiH1yfgSoTODbMi7dwukw6eJfB7L4ouYtIkvYy2yC4UDPMwW0TFePhlfInRm4J5a8G7lhawmNj5N5PNYFl/EpE18GmsRfhgGEMwW0TFePhnfCHRm4JJa8G7lgSwnNj5N5Ls626ilZTo9tqPPeVlYE5bKzdIzZ0wjIqJN23pp+56D0hMan8SkLXwca4HfQDBbRMeo+zQQAJCW+IC+aMFs6jt0UuiRrAXvVh7IemLj00R++dIiN2uQLXgTlg3dS+i+O25INaFJIyZDCafCWBvOu/IFCGbL6Bh1nwYCAHRhDdiVXiveAF4L3q08gIlNtiRNWNJMaHTFZGjhVLU81ob2rnwAghlYATNXv3HxfngDdvX3Jg/gWCoNA0xssiVpwpJ2QqMjJrNedcgLWdpnvCs+EMzAOLzTAYkwc/UBV54F3oAdJz6AY6lUniwnppjYZEvShCWLCQ1WHapR7aNZ2+dafldJQDAD42zZuZ/5OWaufuDKs8AbsOOwBvBaXiqVJesl1awmNiGsXrkoY9KEJYsJDVYdJqPTR7O2z9G7CqGfuQaCGWjD61CHBtlHdWLm6geuPAu8Abv6e/BI6uDDkqrriU3WkwQZXJUxacKSxYTGlUgPRczp9NGs7fOaZVcE0c+yAIIZaCHqUAvamung0dNVv6lVL4NvuPICsQbsRQtmUd+hVxBqYYBaXFL1YZKQhMsy8iYscUF559rO3Kw6hCTmdPpolvY5elcbN/cwf+NTP8sCCGaghWhQWL96ET34rb1Vf4Mn0Q9cLtUitMIetbj8HcIkIesyZi0obfd5FxMSEx7snt5Bqq8jKo1V/62uUKA7H9jFvLYP9jnrNuwrEMxAC1GHWr60SKdPn8OmLU/BpjpzYNOdW0KYJGRdxhC88GmwLeZMTDji14gzOlbiXtuHvQE8oe9TP8sCCGagRdKgAM9iemyKMbwfPlG9DwydoSn1BbpQKlP73BlV9e+DJ4/IzcDqS8xoCJOErMuYd++g7QmJiQmHbEpN3rWz3hvAEstE4jbsi42wCQQz0CLrQSHvZC3GfCALAxyv99GxMhGx698HT56LgdWnthjC6oiojC7adNYebtvYHntMTDhkU2rqXNsGPFs2tb6OSuVyYj/zyUbYBII5xoRBe/kszbuEb9BqYTYlesYQBq6Q8UGMZUlWBjjJM1RZ/3n35EX41hZDWB1hldFVm867M8P22GNiwsG7xtT6uolwDN1r24Bny0rlMm367MrE3/tmI2wBwVyBrEGrhdmUzDOGMHCFSq2IMR5ZGeAkz1Bl/efdkxdR623RFK7adBbODNcOJJtjj4kJB+8ay98yj3bu7U91bRuktWW1YiMgmCuQNWi1MJvKyzOGuhLgUoz5WEdZGeCkw1Yq6983T56t91grEwPbuGzTLp0ZOg6kyra64LJmWn39/MxtToSJCYfoGh3tM2nLrgN0Yvg1IiKa09xg/iEUSWvLasVGQDBXIGvQamE2FfIzVm7aKld8HtJKgMsDAGQHO5fCOisDnHTYSmX9+xSWZHPVy7eJgW/09A7SjmeepUO/Hhb2i7yKClXnSrytHjx62ju7bGLCIbpGJJaj/876+dPaslqxERDMFcgatLwavkpCfcakdD5Eel7yLJYcieyLMdnBzkUYUmUdz2qaxvyObQNcWe8Dx0doSl0djZVKNG9uE7P+fQlLsrki5NPEwDdU+kVeRYWqc8XH1UuX9t3H5ydKZ8tYNmLRglm0fc9B2rSt15uVy7RAMFcga9DyavgqCfUZZdL5qHrJXcas736unx7b8Qujhls0GMgOdraNfLyOJ5YrWxro1Mh5pyLNFxGsgu0VIdU68THMxwYq/SKvEw9V54pvq5eu9yT59vymqLQRed3nBcFcgaxBy6vhq0TnGWUzjNhEJp0Py5DzBvie3kF6ZHsv8zqmPQI2jEzSNWUHuyQjn1Yg8YRHY8NUeujjb5W+Tp5QqVOfVoTyOliyUBU/IU7GklB1rvjUVonce3x9e34b+OpFTwsEc4zIoLW2NtPQ0HDi9/KMyjP6MkgmbdoiqjbkvLIfGDjF3NF88Xsj1NM7aMzzZsPIJF1TdrATGXkT7z6vXhddVOs0zYqQ6YluXgdLFibFT6heeVXnim+rl65tj+rzm2oXLttXXu05BDMwgi+DJM8Y1RWIG4fKK/vu548k3k9FGCaJIBtGJumasoOdyMibePcQHpNRrVPdVS8bE928DpZxenoH6ey5C8y/qYo/XxwOuqg4V+JtdX5bdlkyenoHnR8DrdJXTbUL1+1L1p6HZqshmIERfBkkdYQDr+ysBPMsZIVhkgiysVQnc02ZwY5Xr0TE9eirvHtTXqfdz/UHLTwidPqTzqqXjYluLSw58zYXz2lpoHU3dsiFrlWIBF8cDq6obKtJq7m2SNogbtPjLdtXTbUL1+1Lxp6HOEmEYA6AEGZhPg2SqsJB9VSmOLLCMEkE2ViqNHnNeL0mDTiy7z5q3wUimlIvzkqRxJad+5mfhyY8XPSnnt5B7mRn4PgIbdzco2VzfFtyt4Eo5j5JLLNEQqHA/n7evPI+IToO+qNrFjuxF0ljuylHlOx1TGkNGcdViJNECGYBPgjVUGZhIQ+SqqcyxZEVMUkiqKuzjVpaptNjO/qMbSa1uUE1KSOJbPxsZd1HExTdMh4aZHuqQhMetvtT0mSnXL64cqBqc2phU7SukOH1mSl1fh6ZnAWuxl3RcdCuxHLS2G5q4ixzHdNaI8lx5cuqtAoQzBx8WdrlGdhHtr+oXRYbBimEQZL33KKyd7TPnPh85oxpkxLOR8iKGBkRtHxpkRYXZ+o9IAdbG1RFGUk2dC9xuuQYsaCtmQ4ePV31uUvhYaJ/2e5PMukXq38j/07yvilaV8jw+syFEnslKwSHg0lcOoiyXhWVsX2mJs4y10G2kGQgmDmYXtrVHURF8bW+Bf/LZhjJgqTn5g3wrDAEXRETwqRCBZ7BK7Y2ZZYdY92qhfTgt/ZWfe5KeJjsXzZFp2iyUyiMe5jj+Oz5cY2ukOH1mfbXQ5DyYht0cSnasl4VlbF9psYMmev4ni3EByCYOfCWdnVi+9IMoklp0rbsOuB18L8vmHrutCImT543EwbPtJdh+dIinT59LjPhEUr/Ek12iMpWPD8+hLjpwir7hu4lypkeRH0mT7ZBF5eiLWsHhqztM9Uukq7j2uNbWf/RqaoXSqUJG+pjX4Bg5sBb2tWJ7UsziPIMbMSJ4deUcgGHGDdkglp9bpuYGHBseBmyFB4y7cwH4ZhU76bfSSh7MYiq38+iBbMn7WWIyr6hewndd8cNRCSf6SFrkeY7WYg21YwmplaYbXlYdcuchcc3KtfXtu6biOH32TZAMHPgLe2ySBK+acRadN1Htr/IzdiQVb7bkOA9d12hoHz4CA8fhJBrTHjcifIjIJL6ly/CkVXv61cvmoifPzBwinY/f4RGx0o0tb6Olr9lXqryheJ5Z70f3gqfbtnhSebj0zK97RzIG7qXTFqlMGH70pQ5K1scim0ggmDmwlraHTg+ohXbl1akVs7CWGSR71aHSFAODJ2hKfUFulAqU/vcGU6EJe+5dWPB4/gihHxDZhKRJwGR1L9cDw6i+o/Xe+Ql7ekdnORRHR0r0c69/dTRPlO7jKGs8KhshvSt7HnApwm0ixzI991xg9FnS1vmLGxxKLaBCIJZSLzxbNzcwxS+Y6UybdzcMzEYsZb0WL9TEaldnW20ZdcBZpYGFe9wVgapOn3Y+MzDlbBM8tSnFSwhzZJdUYuTiKT+pTI4pF2x0K3/Wj7MRLQZMo5vZc8LvkygXedANkFI4jMiFNtABMGshCieOBqMDgycqop36x86Q6uuLVLfoVdSidR1KzuMeIezMEhJnhsXwrKrs402betl/i2tQQnRUNmmVicRov6lcmRs2smGbkpKG23Zp6V2EUmbrCvxrexZkOcwNJc5kE0RkviMCMU2EEEwK1HpPeofGmF+Z/fzR5if9x16ZWKDiIn7Z71cxWLCeL58luZdMtl4JnluXAlLWwYlRENlG0wiqpEdHExMNnRTUtpoy77brgje+zHh8FAhBCGa9xUklzmQTRGS+IyI2krlCvqc5oYsi8QFglmRyHt05wO7qMQIaOZtzDMlEnxZroqTZDyTPDeuhKUtgxKiobINJhHVyApHE5ONxJSUP2CnpLTVln21XZX4IOxDEaJ5X0Ey1RZct6k5zQ0XhWdLA627sSPTyZ3s5K8y3PTE8GtetnkIZk14g9HU+to84jTJeCalx3MlLG0Zr7TXFXnnQwWTCDYywtHEZCMxJeXp6v0QUfmI/PcG2yJrYR+KEDW5giQSVVl62021BRdtinXcPa+P27pnfHInO/kLpc1DMDPo6R2kHc88S4d+PcztoLzBaPlb5k2KYb74/fBEgoqhSjKelYNwlKR8rFSiea+fcOU6pRbvfmmMs65RDMWjpErehJfLgdvEZCMpu07Sb117VX0PQXAFz5b2D40YS4FpAlMrSLuf6+faPyLSto211qayEJ1bdh0Q3lO2TKGE70Ewx5AVLyIx0NE+M3iRoCriZIxnfBCODNqmbb20fc9BaYNmyxBmJVxDmV1XIvsObAovVhnWrmi2di+XbcPkcjAvu44vcYKPPvVL5sEgEfGMQ32HTgYjgnRslSiUxuVEOqnsKtmfRNfasnM/8/7b97xERIw8rpRsG/PqhBAhmmhVZvGqJE2q10ef+iXTrhBdFLqyQjiU8D0I5hgq4oUnBrJe0jOBqohT9YjpGjSbhjAr4RrK7DrCh8GIV4aWlukTh2/IXkdG0GTRNkzZEV52nXUrO1JfOy3xnM+VxIV+/BARk+3OxiSc10a37DpA61ZejCuVTUMa4WIiLbPUznpvq64tMkWZ6FqHBtknJB59+Qzz3IPobyJCdELIUtleFlx28Uh20USL1VfSpHoV9Vuii0KXV6aZM6bRxs09RlPvugCCOUZo4sUWOvliC0Q0pV4u1ELXoNk0hLbfPW9QDmV2HeHDYMQrw5ad+2njh6+TuoaK8A/ZLvgcGiNKN8nzXlVfQ67d8fqfrQkg79kqNzQRVYccRGlIeYJk4Dg7Q5NJkvo47+99h15RuhYRUX1dgUqlamU8bv/KWrbRVH/1Lawj3lYPHj098e+kPQtEk/uKTKrX6Hvx50/6bSR0eWU6MfzaRP82mXrXNhDMMUITL6aJDAQrAwhRcr7YaMNjUmPXNWg2hYvNdy8alNPEq2Zh0H0Qj7wyHOZ4q4iq6+rsuQvM77EEWOh2wddVL5WDQnjItDtR/zM1AYy3r4HjSV5QfshB36FXqNg6g9nmymWyHsuc1Md5z8Z6F7xrDRwfkdoIrmMbTfRXH1bS4iSdGhj9Ny/tbeX7Sep78fdT+fyi385pbpioH9Zk/ey5UeZk2ETqXdvUZV0A31iz7ErO534tDdggMhCi5UCVfLEi5s1tZH6eZNB0fyeDzXefNChv6F5CxdYmqq8rULG1iTZ0L5Ha1BK9r1K5PGHQenoHU5dXhM13kLYM89vYMcysukqKv6sk73ahp3eQ/vdDu+jOB3bRxs091ttQBO89EsnHWMu0O1H/MzFLtcgyAAAgAElEQVQBjDauVbYvXjhB5fVF9+a1OaJk+5oWUR/v6R3kPlu01F7ZjnjXmlLHlh/1hQLNaW6Y2Nuy6tqism000V91xzabyGyuv++OG6jYyu4TlX1F1PeI+O/nke0v0qymadzfxUO9ojJt+uxKuu+OG+iVkfPCZ/AZeJhjRB1xxzOH6fDg8KSlAV9T35hCtMxSbGWHWOgONrpeVZupymwuXcsYuq7ONmptbaahIb6XtBKToREq7deHdHG8MqxbtZD5/aQlxEpYAsznsIa0ZOlJEx0U0tE+UyrDh0y7E/U/E95I3sY1EUkhB12dbfTwtn1McWpbXIj6eFIYTeVS+9e27qNV1xaZz3ihxD6zYKxcrlqulxHJlZjorz6spMWRbasyNjophIP3fkbHSlxnAyuGPU7Iq3UQzAy6Otto7YqOScJFNKgQ6ae+8QmegSgUiLtUotv4dQ2abeFia+nahpEwGaen0n59EI+8MixfWmROOFSW/nkCzNewhrRkGZMu05Yq/7ZowSytOEdR/1uz7IrUE0DexrVCgWh2UwNTYMiEHLTPZYdl2BYXoveyaVuv0rX6Dr1CG7qXVF1r+56D0seQ67TFtP3VR2En66yQ6VeV32Glek16P3OaG6hx+lTlvuiDw0UXCGZJxMszeqlvfINnIEQxczKNv6d3sOrYy2iXuE79hChcbBgJUwZdRzD58A5UysCrK12jnyey9qSJ3qOpdibqfyYmgAvamung0dNVn7fPbaL77rjh9RUcuUlB5d+yFBe8uk86QTLO0ZfPcK8lmyM8C6+uj8Iu3lbnt13MksH6rowDSvQd0fs5deY8PfSJt0qWfPI9icJcrYNglkQ0qOimvvEN0RINTzyxGv/61YsmUnsxTx/y5NhLl2E0NoyEKYOeVjDxcnn6lDOXG8KxMttjY33AR0+aaZL6X1phvm7VQnrwW3urPo/6ou6kwEdxwetLlUcyV8JrR6zwR96GsCh22mVf8rHuo3JFZVAJ4SNSs0fR549sf1H69GIf8vPbBIJZEvGgopf6hkdWg6xuzFy88Vd2YlG8W5Ye+OziNsuv12/CjiAJTBl0VcFU2T5nNU2bNMBV5vK0lTNXhywGPx932bPw0ZNmg7id6ukdnJQLNo2dXb60SKdPn5NqX6r23TdxwetLROoZLeLhjywHCxHRogWzMulLruted+wXreJWfke1DqPPZd5rKPYuDRDMAuLCgIXIUJw9N0p3PrBLueFn2ehMx8yJYkdl00HZmDy4jtu09V5NGHQVwRR/DtlcuRFZTpJcD34+5KuWgeXp88GTZhMb/VGmfWVt300hetY0k1KeGA+lL6XB1GFeROxVXN06lHU21MI7gmDmED/ffmLm1tJAp0bOc+PQBo6PUB0Vqnb6sk7ZUUkI7qrRmfY2ieLdkkS4zcHFddxm1u9VhIr3VSXbBIvQwpTSYLqN2Vx5inv68k5W/dFnO2ACE5NS1jV4Gw3zZE+S2gar/69d0Sy9ipvGHsm816z3QrgAgpkDL01QY8NUeujj1YHulUsXYwmbAEVCULXRmR5EZcSTiRRk43+7QngtmcFF9/ldx236bkxkB7q0B01U1m8I8b1pMNnGfPJM5uG98dpx/9CI8qqgifum3S8Q4juII2pXIcbZq/YTUdvg9f//u/t/6OVTr3KvWdmubNdhiO9IFQhmDqLz7Xkked+i34qEoEqjSxpEdQc2kXjSTUE2Kb6qpYHW3Tie3Fx0rSQDUnlNmbJU4jpuMy/GRHWHfJyofn0SgLYw2cZ88Uy6em+2RbmoHVceAkRk9rnS2IHqU1Uv7heo/Dy0yczu5/pp85MvCG15aHH2Ov1E1DZ4/f/4K3yxHP02gleHixbMEl5DltDekQ719957771ZF0LE2bPsU2Fs8/yBl+mVker4zPa5TbTymnbmbx59ar9wK1f0W973zpwbpfevWkh7+4aq/rb+poVUbG2a9NnDW/fR6bOjVd8dPPEqNU6fQl97/e9lIjp9dpT29g3RZXMaq66jguieUb3MmNEw6b0VW5vod25YQO9+2xvp3W97I/3O9Quo2NqUeK1n+44x/z67qYF++POj9Or5MWYZK8vCo9jaRJfNaaTBE6/SmXOj1D63idbftHDiGR99aj8923eMGqdPnVRfPb2Dwr/zaJw+Veq9xuuuEt17myC69wBHZMxpaaBz58doan0dEZWpvbWJrl98KY1eKE2q32iwkGlHOkT1l2VdRfDamI6AEdmM7re+MX1hSdz2Injv7WcHXqYnf/grI3UdiQ3TtqsSXn+ME7XHpPYkU3ei+7Lsexxe3Uf86shp+uHPj1qtN9P09A7S3235aaItN9mXXKBj30Rt4/89f0Rrm3hluyq2NtHIq6P0q1j6w18dPW2kjYT2jnh9dsYM/gmj8DBzSEoTxCLJ+xb9VjSTVIknFXlgeTPSLbsOpGrAJkMLkq6VdBIRD9mysHbNJ3nsdb1rabM0ZOmR5e1cryvQRJJ71TLYDFGxXVdx76cofV5lG4t+t2lbL9f7x/Os+rJCwXtvUdopE3Xtwpse749jJbYcGTg+Qnf9w4+0V7KS7qtiB5LCoXgbcH2Oj5ZdlSXyL1uICB37JmobSYeIxA/IiVZx4/XVd+gk8/eiNqKaii6Ud6QDBDMHlTRBEdz8lLHGm7R0IdvoRIPokePsznVi+DXuISQymBy4k67FMyBJJ03pioikgTrtQJ7GmGS5JM+797zXD2XQwaYAtFlXLDEukz5PRsSLvqO63GkrpEE2JCdNXbuK96/sjxs393APbTItRHXtgG44lC/7JFgkTQJCC1mLSHMCLqttJDmP2iVtsc4eqbyHzqkAwSxA1bDJeg/SehsjRIOoaEaaZjAzGackcy3WO0iabevGTCUZkyw37une24Rw4t174PiIVh7bnt5BOnvuAvNvJuLdbL4n2Swh8T4mI+JF34kGQ9k8v7YGOdlVn8q6Vm2DWXjTdVazXAtR3RU3n0Wn7KpsaJiO52XtBdK5rmrfMul8yMNmYQhmw8iKbFPpd4j4gyjPuKbJuGFK7Ke5lqwnXxWeMRkrlWnj5h5qnD6FRl6tjktzMSDpiAhTwkl0ZHr0eZqcoUTp351MeU28J9ksIfE+JiPikyYmUWaEUrlMvINvenoH6ZHt7BUYEx72eJ+tKxSEp4DptMEsNg+xbNHA8RHuKa5E7oVoZRkHjo/QlLo6GiuVqL6ujvkOInwRnazxxZYtzxqT42TlNS9u5k8+GpuFat8y5XzIi6cagjlweMK7q7ONOxuNG3qVjBPVRs+MEVD9DZH5U9tEHpysvSA6IsKUd0DFs8W7dtRuePXY2DDVmOG0Kbhkl8XjfUxGxMtMTFiZEXghHXFMeUTjcdmiutZpg7b6dxJxW8QL04jIQoiy7OWdD+zifn9D9xIvBAlPMG3oXkKf+YNr6bEdfV4dP02U3iNqK5638rqqR2Or9i1dR0283kR2YPz/w/A8QzDnmHUrOxKFQ9IgG8957Mss0YYxqjQm/UMjUr+Z09zg5Nl1RIQp74CK94117aQ2plMmETYFl+zkIS6mZES8zpK7TEhHhGzqMlODV0/vIFdwJr3vyd608Y2S2/ccdDqY8t5H0xum0gdvvsqbQZ0naoqtTd6UUSSYvnL3KlpcnOm0PEn4NNaZRmXs5PWBs+dGmXuhePVWKLCvP3B8hPn9LbsO0Csj570T0BDMDnEdwyMjHJIG2f6h8eXgpFmiLw06LZExufOBXa8vfYs5dcZd2kPVSYLJ0ARZ7xvr2jJxv6aXt216d4gm96lFC2ZR36FXUu9b0AkLkAnpiEg6KCh+uqmMSBBl4xEdm66Td9i1aOnqbKMDA6do597+SZ+zwrKyJIT8t1nuwdChFsY6GXix06yjt4n49TaFEzbE+1x0SnKWQDA7IivjnyQcZGIyk2aJrk4hdInu0rsq8Tpav/pqY94WmwOpyrVl2phPg3sS8T7V0ztIfYdOvi5s+epWRsSrhgXIhHRMra+jj65ZTETig4J4p5uKRALv3YrEMpHc+/ZBtOik4XJNViEsKtjcg2FjnPH9ZFaXdHWOZ4hi9el4P+DV24USO8ae93nSfbICglkT1SNKfTD+LFRSFfFmgzqnEPqOztK7quFm1dGD39qbGHfIug8RPw7MxkCqcm1RGyu26uVx9gXb7TypHcqEdFwolWj7noPczCSRDdI53VQn1RmvfbP2UqiUp5aFk60VFVPY2oNhq//5kvfcF2T7Aa/e2l/P1x8fL5IyXvHukxUQzBrIHFEa76y+Gl6VuEnebJBl9HydIMiiuvSuY7h16kh0H969bdW37LV5bcyXDUlpsN3OK9thZWYE1oEx8e9GoRyVmwZZRDZoQVszHYydAkYkFgncLAfNDUzhO34SZDUyce6i8vgknEwL91BX6uLlXnVtMTF0qRKZMdNW/wshzMUlsv1AVG+88UKm3/syUYFg1iApJpPVWV3MWHUMK0sYnj03yhzseLNE1j18nSCooCI2dQy3Th3J5gFOurdLQlgy1sVFO1dph9F3k0I5KolskM7pprx3S8QeCEfHSkqxjyxcTtB1DowxKdxDXanjHfCjMkmWGTNt9b882ywdZPuBar3Fvz9zxrRUeaZtA8GsQVJMJquz2p6xpj22OR4TpjpLjFNrS1o6hlunjmTzACfdWxZT3i0fl4wrn21W0zQiIuWd2b62c5V2EtkgndNNicTv9pHtLzLDuGRjHyPq6wqZTNBVBYBp4Z7FSp2JPm+i3DJjpm7/kz1zwDeblRUq/UC13lj6w9eJCgSzBklxe6zOanvGatKwmigrz9gtWjBL63Q430k69IT1nDqTKJWYURMbEk14t3xcUo4/m0wOcha+Lt3y2smc5gZqnD5VmKnD1Lvp6mzjHmMvG/tINB7nnnTsr82Ji0qdyAp32T7heqXOVJ83UW6ZcUin/yU9o85eFN/smw1cTSB8nqhAMGugshGnEpsNwbRhTVtWXgxwZYqmLJYXbRm3pENPWM/JqqP1qxcJs2SoxJynFW0mJmG+LinLhADIPKevS7e8drJupdvT09LGPo7/Lbkd25i46NgKmedV6ROuVzBMOV5E5VbJDJQ0Dun0v6RDNFTsla/2DdgBglkDlY04rvBxaThu7DZu7mF+z1Wsrc00RZVtgnfoyZZdBxKX/ZJObuINEAcGTtHu54/Q6FiJptbX0fK3zEu90Uj30IlKfN38KROyMHBc7vAaHz0ivgh51djHyiwZKscjqzyvTP/WFUIyz6vSJ1yvYIgcLyp2UbTCqJMZSISo/7HKLHpGVXu1ZdcBpe+nxWYaUteE6JmHYNYk7SCpmhosCdeGVaexmzLGuthOUxS1Cd6hJyeGX2OejqQKK+ar0nM/OlainXv7qaN9pta9kjIWqEzCfN38KRPaUi6TkfeVFT4IeZuxjyq/r0wDWtkzef2bZyse2f4ibdrWK4x7Hf89/3lV+oTpiU+SneX1i5kzpinZxXi5Z84Y3yMQPwQmwobA5NlyXgaXyy+ZQUeO850E8bpbtGA2NwWiDfumm4bUR0L1zEMwG8BE/t2k1GCy93PhUUoq/45nnqVDvx6uKpspY6yLyTRFoncgEmM2BgZXG40iVCZhPq58EMmHtmTtCc8DWQt3mZR1cSHMsxXRBsbIRh0YOEUfvPmqSd9Jel7VPmGq/mREiurx7KL+EZVbpv5lV3NU4Nmx10bHmJ+L8gKzxijVfUxp8XW1TodQnwWCOSUm8++yv1st1Hhnr69b2ZG4OcYEssfhxuvCpDHWwVSaoqR3LnpOG54H015cUbiCqjfD101x8QnmWIl9Ql/WnvA842pJVsbexoVw0xumSh2BHXlMP/WBa6XLk1WfkBEpPMeL7OZNlftWkrSaY3JF88y5C8Kc0CpjFA8b79LX1ToRvPcW4rMQQTCnRnWmJIoNZRFvQLz78c52t4HqcbhRXdgwxiqYSlOU9M67Otu4J5bZ8DyY9uLyrldsbVJuW77E0rKobJO8vMVZe8LzisslWZUUexEyYjli595+umZxm3QsaVZ9QlaksDzaPM+rTPo22fFONGbqtBXRSl/foVeYziXVMYrFnOYGK+/Sh9U6lYmL6L358Cw6QDCnRGWmpHKaVUS8ASUZfxdLGqrH4fYPjdCdD+ya6GBxQ6VrjHnwOrXMQCUjqnnvoH9oZCKF3LqVHc68SKY9Vqavx1tSNulhnLjWy2dp3iXq1/LVEy6LLW+treuKVqlM30/n+G5VtuzcTxs/fJ3097MIU0kSKaJ9NQOc+lNJ35YEz0Giu3yvu9KnMmFgsW5lh9T3VNG1Uab6sOrERfTeQrW39ffee++9WRdCxNmz5zO574wZDVL3frbvGJ0+W+2NaJ/bRCuvaZ/02cNb9zG/K2L9TQup2NqUeL+IM+dGqfutb1S6hyqN06fS3r6hqs/nNDfQq+fZ8WFlIjp9dpT29g3RZXMaJz0T73rxZ5ch6tSnz45OuufIq6P07R8coP/3/BFqbpxK71+1kG5fvajq+sXWJrpsTiMNnniVzpwbpfa5TbT+poWTDILoHUT3u27RpXTdokuF12Eh2+5Uy5zl9Vjw3lO8bShfq6x3rbTP3NM7SA9v3UePPrWfnu07RkdePkvf/sEBevSp/bT7p0fo3585TP+267/p2b5j1Dh9qvIzJt3bRF3G257JdxTn0af2EysI5tXzY8bvx7MvdQWiKfV1zA26qgyfPW/d7qZFZGcHXhc/8brf2zdUZevqCkTtrcn9Q3W8Y42ZRPy2kjTWFVub6D9/eoQ5JvHuxYNXd6uuLdLohZI1O1kJy0ZteO9v0Fv+1yXc35jsw7z3OXjiVeX39rH3vMn6GJMEb6ydMaOB+xt4mFOiMlNKig0lqs5bvH3PwUmbUZLigF0safA8tURy8V9xz4DJJUrerFYl/3OS90cmFnv7npfovjtu0J7J66S1S/pO0nV5f48+F2UHUMXkpg9T19L1+vGOAY6QORQljRfI1gYamxtzVLy+ae8nsi8qXtAZ06fQmXMXmH+b39Zc9ZlvabNE9cBL+cli3tzJB8moxqny4HkX0yzfm1rp8yW0LG6jktKQmuzDqnHHSe8t683AOkAwpyR64ZPyhzazZygysaHR//OWPzZ0L6EN3Uu4MbKuljREjf3/7v4fOv7Kq9zf8tInmeg8KkZadyCuNJ68nMu68de7n+u3Etspc7oV6+8HBk4pTTayOMFM51omxYzKJt6LvzGTypDI3mYgmxtzVDYAy95P9E559oUlhOIHLEX8we8squoPEetWLawqi49ps3j1oGI3ZTdAi8a78YwU43U+v62ZVl8/n1svMk6pNGF4ScSvfefazmCEHi+URicrierEJdSwCxEQzIaoFK+8DXgqDUg0M4w8l2nOXLfp/RCJZSK7XnAVz1WagT8yxrzNYqIjsUVs2bmf+bltbyHv77ufPyJdnqxOMFO9lmkxo7OpTCeVIQ9bG2hsHzdNNFnInD03qr1RlvdOH962j9rnzhD2Q5aI7GifybStXZ1tzL8tX1qc5OkLLW2Wit2U3QAtGu8q61z3sCbZCWcaZ4yvE5+I3c/102M7fsEdx6fUF2h0rDowYkpdnfK9VAWwL155k0AwG0DWOKo0IBnvjsklZFNGQMbbZnOGqeK5Yh3TamqzGJFevR4aZA8ctr2FSXlnZcqT1QlmqtcyLWZ0NpWppjIUYcuTY9tDFLdfvPAImfvx3mm5XJ16U+Ydi2yrjN0NLW2Wit2U2QB99OUzRgWTqM6TjrpOY991D7Bxgcw4foGTLnOsxLbr0XV1vPVJv8sDEMwGUD25KUuvEZFd74fI2xYtxdnsQPFOXVcocEUf65hWVYErE56hUq8L2prp4NHTVZ+rvHeW4UpqT7y/T62vY9YfqzxZnWCmei3TYkY1v/j4b9RSGYqw5clJiv01vUKV5jlkvPwuU2+GljZLxo5Nra+jj65ZPKnufIhT5b37geMjqe277AE2KtfUgdXfZMbx9rkzmO9n3lz2hj9db73vnnhTIEsGB5VsBSqZMmQxmTkiju6uYxl4dVFsHd8oYjI7AI9i63i9d7/1jXTp7Ddwdzf3HTqptOs36X7bfnQwdb1eOncG/fhnR6s+H371PO2VyLDA2xV9/eI2+hVDiEftidfeVl7TLvxdJbx3P6Wuji6d/QZmRpLoPa28pj1V24iudcd7fpO6FrXSwNCZSVkrKuvNdH9l7V6/fvGlE7vnZzc1UF19gUYvjA+y9XUFerbv2MT7vGr+LOm+Hs/GET2Xibpk2TzWdW1mz9B9jqTsQZWo9m8Z4nVn037bIqr7y+Y0Msv+R7dUx+6aeE6dzECV8N79VE4GFN77Z/WtX584I9WubLSpynKx+tswJ1d45Xij+n5UM2FE/O2//ZSZjeRnB16mJ3/4K2Z2IJ4tc4VOlgwIZg6syuS9YBvG0WZqLxsCP4JXF1PqCzSzqUG7PnQ7F68ef/e3rjA+cUhTr9Hz/ftPDtHspgZ6w/QpdC5mgGSECc/gjV4o0fqbFnLbk6ieZNsh792XymVjgiqJGTMaaNezh4WCzlZ/rRR6v/m/Lpn498ymBtqzb3Diu9EYrpqCkDdwbv3Rr6QmU0nIChfdQdUmvHfKwkbqzXjduUjNaAuVspt4zrSCmffuy0zrzn7/qo4GmWsmITum8fobb0JQOd6ovh+dMbGnd5B++PNqJw/RuO1n2WCbk25ZkFbOIjJLDjaWRG0YWJuxiV2dbdTSMp02P/mC1EZIGdIu9/Dq0fSyaZrE8pW/q8y2wtoEJQrxSIopFNWXKJOAbD0Tjcf3scI4XG14kjmJMfq3TH9NG36QFNe/ZdcBapw+peL67LLIxOkS2VkCrawDXt5inZ33pqh8pwPHR0iUWtlVWERI8ZusNs46CY9F1s/J688qB2Lx+lbfoVdoQ/eSiWsTjW/olrmmCJUxjWfTL3DikOPjDev98GyazgbqR7bLn4KYtMnc102xERDMksgMwq5etG4+3Qjbu1eXLy3SYzt+oSz2eNjqXDZOtCOSSzFYiei4cxaiWFsTk4A0ArGrs83ZUec8VDfMivJMm4jNS4qvPTH82sS71hk4K5HtE6x3vHZFdS7h6LsyMdrlMtEdD/xHYlYKW8TfadapN0MhD/GnvPE37RkJlY4GUT9QbVMqYxrPprfPbaL1qxfRYzv6lMZx0ftWGRN1Ti5O2mTu66bYCOeCuVQq0b333kt9fX00bdo0uv/+++mKK/w3YL68YN18utHfI2wL/Kzz7Mpga+Kg6llXTUsmEr9pJwEmBs+sNzzx7l9XKEw6ol2mv5iYrOlk0VAZOCuR6RO8Z25pmU6LizMZZTkoVWYiN95uGSpFjq9prVQmpjbTgIbq7UtCxb7L2CxePc1pblCuJ5UxTWTTly8tMvusiKSUtdF/J9WZTv75pE3mvm6KjXAumJ9++mk6f/48Pf744/T888/TF77wBfrKV77iuhjK+PKCdfPpujZ+JutrVtM0pqdoZtM0rbJVYnrioFP/vLrihWSIxG/aSYCJ9pN1wnre/eM723keSKKLz2tisqaTRUNl4KxEpn/x3vGWnftp44evq/pcJ8/0+H2yF1xZhwvwUJmY2vYA++IMIjI/MZB9/zI2i1dPp86ox1+rjI+mHTtJ71u2znTsQlSfWY8RujgXzHv37qW3v/3tRET0lre8hV544QXXRdAiqxccNyADx9lGLKulDt7SrpP6EsQpZoVO/fPqat3KDiJSN5QqIkG1fclgO+RH9f681II8sUx08XlNTPzi8bVT6uporFSieXOblA7rkInTlelfvDZ6mJMDXMdDTpRtTLPvqExMbTtBfHEGZRkaImOzTNaTziEgpuqA9xwzZ6g5oESpSD+6ZjERXazP6NqbtvXS9j0Hac2yKyfFhvu2+sPDuWAeGRmhpqaLuyDr6+vpwoULNGUKuyizZzfSlCn1roo3idbWizF9a1c0U0vLdNqycz8dHhym+W3NtG7VQlq+tGjt/qxjknnMb2um1tZmWnAZO49v9Hfb5YuWdteu6DBWX69wZvCnzpw3/kxp0an/pLa1dkWHlbLqtC9Z1q5otlbuJFpbmyfuv/u5fnrwW3uVrxE97/rVVzN/v371IiP1wSsf7/pJz9XSMj2xXKptlFcHSZTLRC/2n7JqI31CpT0ceZk/sY5fR+W7cXY/109bdu6nQ4PDtIBjg0218TS0tjbTjmeeZf5txzOHndiSJJtlsp5M6on4vXc/10///L3eiRN35856A/3h2s6Ja/Oe48Twa0r9lXedT61fSkTjK1ZHXj5Dc1qmTzr9N9IJn/mDa+krd6+Se0hLqL4354K5qamJzpy5ODCXSiWuWCYiOnlSbzkwLazjOhcXZ1YtWYqO9EzLYzt+If3d1dfPp6GhYVp9/XzmzDX6u0l45duycz8tLs7Uqi/moRuX8Gf2NutfB936j+qqst3Zfjad9uU7lfWnsyklInrexcWZTE/I4uLM1PURtfUCEU2pv+h1lrk+7909tqMvMaaR10bXrVrIvCevDoiSs1LIlEcWm3G8aUk63jmOik3TtX/x9n/w6Gl68Ft76fTpc5PqzWYblyGqu0O/Zt/r8OCwF7bHdD2Z0BPxdseyecdfeXXSe19cnMkN91Ppr7z6OH363KQyVIpl3XvZgNdnRSLauWC+5ppraNeuXfSud72Lnn/+ebrqqqtcFyEYeEunhcL4DlnWUobL5XDVpd0keEtyq64tMgeMaND2aSDVqf/K8i+4rJlWXz/fSfl12ldI6GxKKbY20aIFs6qyZsim2JIl3tajkJFFC2alih+UCZ3htdHlS4vcAVuUcpBoPDsGSzSbCgXLQyaHSlSW5HXD21RCOdKEcbHsrY5NFoUKbNzc441997m9iWxe5Xt/ZYS9aqvaX1n1sXFzj9Rvfc+IwcK5YL755pvpRz/6Eb3//eF1TjkAACAASURBVO+ncrlMn//8510XIRhE6WREA7hsyixb5ZvfpreMJ5sLM35Er28DqergE/cCZX18b1L7CgXVTSkbupcQETlpT7y2vnNvP3W0z9QWFyrHaZt8Ht4RvKbiYH3ZzGwKlYm1rhPExn4WGXura5N5EwPZtIs+kZUTR2TzKt+7zbh1Wbvre0YMFs4Fc11dHd13332ub5uaLDqArRRhBwZO0QdvTu/Z525WW7VQ63o6h27wBtKvbd03sbnAZ8OapRAIdaeyLLxBodja9PrBBtUChOcdkXkfKjZCNKjI3CvLd8d6Tpvl6ekd5MbXDxwfobv+4UeT8p6vW9nhdZ+PUJm06ExwbIgiGXula9NYEwPeplifJ0pZOnFEG3Qr37vN/iq7SZiX29mX1WIWOLhEAlsdIJ5cP27sbaUIk/ViJaGztCtCx8CLhEcI3og0XqC0xiXrbBa2EQ0KPAGi+z5UbYRoUEkTVmH73fGec0P3Eiu73pPi0Mtl9bzntYINUSTTP9KGC1W+tzsf2KV9rTiuxJiPTpDxv1187zbtB68Mc1oa6NTIee69fFwtjgPBLIGNDsAaCFjGPs3SqUhMmvLAmlza1THwMrPZrLwRMgZa1wtkyrj4HpOXBp1BQfd9qNoI0cCWVViFDEmHHpguj04celSe6Pe+eqtsY0MUyfQP3nfGSmXauLlH6T2Y8pK7FGO8cXfg+Ij1WOzoepMccS0NtO7G6lUXW/ZDt92FEHYFwSyBjVgw0UCwZdcBIw0kSUz6NoPT6WgyhzlklYBfxkC72NCTJ1S9RKqDgu77ULURXZ1tdGDgFO3c2698L1nSetRYv3ed51006S8UiJudY+D4iPfeKheYFkUy/UNkk1XfgykvuUt7yRt3o1Mwiey2Rx+cIDpl8OkAHR4QzBLYiAUTDQQnhl+jnt7B1I1e9oQxn0SWakerFNn9Q+yDEnTeU1qxIWug45OE+W1yWTJ47ad/aCR12/E1jkw0CSF63Zv48lmad4l+mXW9Izo24oM3X0Ud7TOtLIum9ajxfs9LR2VrA48oDp2ozHUITKmrYx5W45OtCxGZ/iFjk2XfgykvuUsxpnKyJ9rjRXw5QEcEBLMENmLBkry/JjqSyItViU8zOB0ikc2Ld1R9TyaW71QMdOUkgZUbkpmbWtB+oo2dfYdOKoten+PIuEc6x464Zm1uVZkE6HhHdG2ELW9QWo8a7/enOOmobG02TKpXnjC5UKoWy0Th2zofkGmz0XfufGAXlRjLAKonh6btIy7FGEvk83KV+9AefXGQhLAJHYJZAhuxYEmzUFMdqdKLxZvt1xUKdOcDu7zyJupg6j2ZWL4zdfyoam7qiMpJkoroNfHstgwwbxLCO+I62txKZD9VnG8bKNN61Hi/H3t91E/awGOKpHo9MHCKfvBfAxPlanrDVPrgzVfR9j0HvfdW1QJp9miYtCGuxVhc5G/c3COsh6xEq28OksoVLF7sdZZAMEti2hMUXeufvtdLY6XqqaeuYed1PJEHNlq6zLqzmMDEezKxfCfKKaoSMpGUm1rlJDsZ0Zv22W0aYNl0RZWMb/5iB7qaXg71IXYwIq1HLamuGxum0kMff6t2+WSI27I713ZOqt+e3sGq1bORV0eJKAxvFZFZoeSLp7ASnfdgw4ZkPaEV1UOWolW0aidqS6bbGjMJwmm2IyRLpATzM888M+nfhUKBGhoa6IorrqCWlhYrBasFogbG6kiLFsxS3lGb1PHiRqOuUFCK8/PRINvAxPJdV2dbVahAhIpQS8pNzfOk8X6TRNpnt7m5hpuuiBNXSzT+zLyNYTaWQ33pI2kFo6sVMBbxdJtEbBGRlLEj+m8fPP4sTAol3zyFETpC1ZYNyXJCK6qHNLnf0yJateMdFmOjrYWyiV1KMP/DP/wDvfDCC7Rs2TIql8v0k5/8hNrb22lkZIQ++clP0tq1a22XM7ewOtKiBbO0ltRlGl2l0VDJcWnbIPsiNIjMeadMHD+aJGBVNpjIiN60z25zcw1v0CHix7KOPzN7c5jp5XmfREtaj1r0vUe2v8icVNsKbUjKu1xpy5Lamk8efxYmRYLPgkP1PYSQLUEHXj1k+bwqq3ZRW7LR1kJ551KCuVwu09atW2nevHlERDQ4OEif+9zn6Jvf/CbdfvvtNSeYTYs7VrwTi6QGqdroVLyJNg2yT0Kj8p5pvVMmPNVJAlZmwhX/TRJp4shsb67hDTpJKdpcLM/7JlrSCkbRCpit0IakvMs6x/vq2GsXE3hTIkF0EqJvgkOGELIlmCTL51VxuERtyYa4DeWdSwnmY8eOTYhlIqK2tjY6duwYNTU1UZm33plTXIg73Qap2uhUvIk2Z4C+CQ0iM94pE55qGfHOKqtOujITcWRZxY5Gm1t3PHOYDg8OM585zQRIRkCF4iVRwXXspyjdJtHkwy9k2pqOvXY1gTexMTjJI++b4JAhlPhzFjoTrSyfl9W/eceRR23JhrgN5Z1LCeZrrrmGPv3pT9Mtt9xCpVKJtm/fTkuXLqUf/OAH1NjYaLuMXuFC3Ok2SNVGpzIY2pwB5lFoEJkTGzriXec3Mm07aUDIcnNNV2cbrV3RMZGWr6d30MjJWjwB9fC2fdQ+d8bEdXX6iE+hSDxchjbILBGrHMetY69dTeBNbAxO8sj7JjhkyHqDni66E63480YTpk3besnEabxJxPt3UnpWG+I2lHcuJZj/8i//kh577DF6/PHHqb6+npYtW0bve9/76Ec/+hH99V//te0yZkp8QBs4bn/pK01OVyK1Ric7GNqcAYayHKOD73GUlSRNXGQHBB+e2aSXkCdKopO7ouuq9hHfQpF8QPXQh6TjuHUm464m8CY2Bos88hu6lwTbjnywIaqkmWhFz+uDTUjSEbbEbQjvXEowT5kyhW699Va66aabJkIwjh07RitWrLBauCzZ/Vw/bX7yhaqd2jxMirs0DdJWozPdSSonIrOa2EuQIXhHQvAQypI0cfExdIZo8jtYcNn4SYkmy5oUJhBdVzUzg6/1KcJ2e2fZmTSHPuhMxl1O4NNuDBadhGi6DeXJ1vFI84wmJlqmbYLu8yTpiBDErQ2kBPNXv/pVevjhh2nWrFlUKBSoXC5ToVCgnTt32i5fJiTFhbEwLe5YyyQmlpdNlkmXeP1WbjA7NXLe+ZKULj54A2SQNZpJHlIfQ2fi7+Dg0dP0ta37qFBgf1+nrDJhAjqZGXj1OXB8JPO+zsJVe2dtgtYVsDorYy7jKdOKc1dlDcXWpSHtM5qYaJm0sbXwzlwjJZi//e1v09NPP01z5syxXR4vSIoLKxSI2uc2SXta087Ms2z4NrwKvPptbJhK627s4D5r9FtfhEQIHkKVtpO0iuBj6AzvHUypqzOWDk0mTEDnurz6jEI9iPwa5LJq72lEoW6YmupvdEkreF2VNQRbl5a0z2hi8qJiY3t6B2nHM8/SoV8PM8dD3vM8sv1FIsrenoSIlGC+/PLLaebMmbbL4g1JS7Dtc5smll+TMCF2szJWtoS6aBYtOnko6SCDOJHYHxg6Q1PqC3ShVJ7YpEWUXnz76HGNo9p2RB5Sl94s2XfDewcXStVimYjo7LlRpZMWiSaLEl54gE4dqMbrZj3AZdXe04pCV5tmdTAheF2UNQRbl5a0z2jiXcraWJmxmfc8o2OlTCbheQjpkRLMV155JX3gAx+grq4umjbtYrzpn/7pn1orWJYkLcGqDI6qgoXVqLIyVraEumgWfYSzqZJ3kpuoHisNyujYuMqJe6zjn6k8l48e1zgm244Nb1a8vS9aMFvp0B7eO2if20Rrll1RNdE6Mfya1ruuFCXjZU5fB7rxulkNPLOapgnTTekg+yx5jpkM4dlCsHVpMXXCa5p3KWtjZcbmJB3jchKel/AQKcHc1tZGbW3hPFRauEfwKh7iQKQmWHiNinf0r21jZUuoi2bRKkc9E43He7JICqth/0bNgOh6XONH/85pbqB1K9XalejalQLEtMgxObiz2jvv3fPejegddHWOn0qV9njyOCbrQDVeN6uBp6d3kDtpjec9lhXzeRlEa4FQ8uSmwZdnlLEvMmNzlsfbx8lLSI+UYM6rJ5lHV2cbtbRMp8d29KX2Ipk4TY+H7Y5sy6uQNItW2XBZLhNziV0ms0EcVQOi43FlHg5S4fVcu6KZ+7skIcISIDx8GOhU2jvv3cTfwfy28SwZScuSvi4lJw3aWQ08vPvOaW6Y5HlXEcB5GURrAZdx3VkR0jPKjM1RuV0fb8+CZ4f7h0aUQ+SyRCiYb731Vvrud79LV199NRUqtp1HWTJefPFF6wXMiuVLi7S4mD5u28RpeqfOnE9M0G8DmzNu3iy6q7ONHt62j7kszYM1wMpkNoijY0BUvY0ikbh9z0u0dkVH1eeyQkQkahqnT/VuEFCZ1IjeTeU7aG1tnji4hCj7pWTV8ImkQTurCYDINkWoCmAfJjOmwlt2P9dPj+34RdDxmUmEEDqSllCeUXZsFjmhXDpNRONxSKtKQsH83e9+l4iIfvGLX1T97fx5dv7IPJLGqKrMWkWDexYdOasZd/vcGUpilzXAqmyouvibagNiOl5UJBJ5QkFWiIhEzUOfeKt0GV2hMqnRNe5ZLrOmOfmL9/esJgAy91UVwK6fRTZePn6Co8x1EVrijjxsHktL9Lw7njlMhweHhWOzD57zpPE4lFUlqZCM973vffT4449P/LtUKtFtt91G27Zts1YwXzBhDGXFri8xVJVkIdS5MeQKsdzxzAZT6uporFSiea9vBov+JjqK1MZAKBKJUTniyAqRrL2pqshOalZdW0wVb0ykP1jwBmeZQdtGyEFWNkLmvqrtz+WzqMTLx09wTNqcXV/HvmcoIiAkanVywrM3a1d0TFpR45G15zwp3NLXELk4QsH8oQ99iH7yk58QEdHVV1998UdTptA73vEOuyXzBJdxdrqDe95m3Lx6IFJbWkoyEkmi2KXgIRqPZd79XH9VKJCsEPFxwiUi/p7rCgVmrF3foVdS30fnffHaxoGBU1KZPGyEHGTlLZK5r2r7c/ksOpuAx39X3dfj7aI0xv5tKCIgJGox7l00RvH2vPhItAk7JKdOHKFg/sY3vkFERPfffz/dc889TgrkG67j7GQH98ocw5XhvnmZcYvqweQAKzLANgXPP32vl8ZK1YHaW3bup40fvk7r6HAflt5UqXzPdz6wi/mdrIQHr23sfv4I5/uTB20dj7/M5Pfiez5IR45fzF3uQjSnib/WuaYpdDYBE7Hbnqz4DkUEVOK788WHuHcidnhP36GTVupNNEax9rz4TGhOnThSIRmf+cxn6KmnnqIzZ8Yb5djYGPX399MnP/lJq4XzAR+XuWWO7s7rjNv0ACsywDazhGza1sv82+HB4cSjw5Pi1UJ977L17WpQFyX+ZxEftFUHB9nlZt73VGNvbeBT+4uHTfA8wSJYfV1WfIciAiJCCHewYZNV7UlSeI/pevNlkhDhak+Xj0gJ5k9/+tN06tQpOnToEF133XXU09ND11xzje2yeYGPMyIZD4dMZ/Ldm+ACkQFes+wKa++ed9/5bc3c99vYMJUe+rh/m/dMIdPXRGESH7z5KqPl4b2jqfVyx26rDg6yy82878nE3tYKsmETEYUCSZ/gKGoXpXI5OBEQEUK4Q5KNMCF+k/qP7ArD17bum7QnRhefnHYu93T5CGe7wmT6+vroG9/4Bt18881055130mOPPUYDAwO2y+YFXZ1ttKF7CRVbm6i+rkDF1iba0L0k0xcu4+FI6kxRw+8fOkOlcnmi4ff0DpoqZhBEx2RXf36F1XfPu++6VQu98yi4Qqa+eYPVzr39xtsu7x0tf8s8zverxVVXZxvdd8cNtOmzK+m+O24Qth3Z9y7T/7fveSnxO3lGJWa52NpEm/+/d0j3dV67+OiaxVLv2VdCsDsiG6EzpokmCTxUwntMjKuiMco1OvWVJ6Q8zJdccgkVCgV64xvfSH19ffSe97ynptLK+TYjkknHldSZeA1/y64DXj2rbZK8gLbePeu+ixbMoi0791OJk4TalEfB55WFpPoWDVamY/pEbaOjfabxZUVZT5JM/08SOaw2ENIGoiRURE1kK2X7OqtdrF+9yEje/izxyZMpgveedDzkOpMEnRz/aU8Wja6RdRhD2kmVz2OPDFKCeeHChfR//s//ofXr19Ndd91Fx44dm3SQCXALb1mqrkATadOSGiGv4Z8Yfi2ok3dMkNWEqPK+MnHpJjwKIcQpihANVv1DI8wsI2ngtQ0bbUY2/EsmHV/SxkJWG2hpme5c9NkaQHntxNQhPvH3Hz8wxxY2BYeP4Ycq8Ma0geMj3N/oTBJ0cvyn9dL74rRLM6kKfewhIqq/99577xV94X/+53/obW97G1166aX0pje9iS699FL62c9+Rq+99hrdeuut1gt49mw2nuwZMxoyu3cSxdYmumxOIw2eeJXOnBul9rlNtP6mhfTxW3+DVl7TTsXWpsRrPNt3jE6fHWX+bfDEq7Tymnbt8vlcdyJ6egfp4a376NGn9tOzfceocfrUxLrU+Q2Lh7fu476PYuv4+zVhVHj3SfvOXdE4fSrt7Rvi/v3HPztKl81p1HoHWcPr1/H3Xvm94VfZ/Wz9TQu5dcBrA0eGztAKTriJDaIB9PTZUSoT0emzo7S3b8jI++O1kw+982q6ffUi6n7rG6VtpQwubJ7N+iKSb3+mMVV3ojGNV0e8diLqP6x6mt/WREOvvMotW/vcJiv2dcaMBtr17GEjY5AMOvUV4dvYw2t3M2Y0cH8j9DB/+ctfpkceeYSIiP7+7/+eiMYF9Pe+9z1aunRpmrKClKTJLRulo+PhU8yaK3RmvyZnzDzvSH1dge674wala+ncJ5R33tXZVpUHOY7NTUq2lxRVwgIqVydUlmt5beDwoH0PaSU2N5n5tIxtCheb8nzxZOog8vzy6ki3ncTraePmnoSy2fHS736u36nXNk2/Cn3sIUoQzE888QTt2LGDjh07Rn/3d39H//zP/0yDg4P0pS99id7+9re7KiNIQHYQl1n2J/IvZs0FOoORyQHMVfxgKHGKIj5481XU0T6T25b7h0bozgd2GRe0vi4pqoocUYYWl9geQEMWfyzyIDh4mJiIdnW20cPb9jGznYjqyEQ7EcXM20wSsGXnfubnNp0GuvWVh7FHmCVjxowZE6EYP/vZz6ijo4OeeOIJiGWPUNkZLLtzPJSYNRP09A7Sxs093LhYkaE1OYC52gnt047rNHR1tlGxlW9obWR+ycsOcVGGFpfMm9vI/DykAdQlaesrsnV3PrCLNm7u8SYjUuQlNZGxqX0uuy5styneuym2NlmdtB3irAr5OInKw9gjFMx1dRf/PHv2bLr77rupvr7eeqGAPCqDuGgW7EvKPJdUTjZ4iAytyQE/Spd05eUtVt+Fj2kSdeEZ4DimBG1ePHy8NrB8adFpOfIwgLokTX35nEZU5CVVJas2ldV9F3BWhXycdOZh7BGGZFRmwpg+fbr1wgB1VAZx3pJIsbXJaJxsKMh43EUGz/Su8q7ONlq7osP6bvu8LFXH4+lYR40TmRO0JpYUfUmr5EMbyGOcsU3S1JfPh5KY9JJm1aayuu+6VQvpwW/trfrc10mnD3YnDULBvH//flq1ahUREQ0ODk78d7lcpkKhQDt37rRfQiBEZRAPPW2QaUQe92Jrcno+DPjZU2mAeaE1prwtafuPrzHQWRL6ADoxAXr5LM27xP4ESLe+fF4dWdDWTAePnq76XLff+pAm1BXLlxbpv14cpN3PH6HRsRJNra+j5W+ZF3Sf8hmhYN6xY4ercgBNVAbxvAs8Ve+dCY976AN+nrA9IUzbf3z28gF1QpoA+bzhKjQvaYQPq0W7n+uflDFodKxEO/f2U0f7TO/aYB4QCub2dv/zstY6qoN4XgWezuAFj3u+6Opso5aW6fTYjj5rE8I0/cdnLx9QJ6QJkGtbpyImly8t0unT54Jy5PgyWcoiS0YtI3XSH/CbvIpgFXQGr7x73GuR5UuL3h5R7LOXD6iT9QRIRZS6tHU6YjK0McyXyVJIWTLyAARzTvBheShLdAev0Ax11tR6O0sDVjTs47J9yk6AbJTJZ1Hqi5i0SdaTpQjT8d9ADARzDuAZzy27DtC6lR25MVIi4L2zj6idvTJyHgI6Aaxo2MX1MrnMBEi2TKqimidKt+w6UHWd6PuuJrm+iEmb+DLehBr/HSoQzAGQZEx5xvPE8GvebkIxDbx39hG1MyK/Nz35ggsvX62uArj2bMpMgGTKpCP0eaL0xPBrzP6ocu20+CImKzHdJ3wZb0KM/w4ZCGbPkTGmovRoRPlaCuMB7519ktpZRC20N1/xZTNSHBciPgvPZjQBam1tZuZPlymTjtDniVJZbPZRX8RkhI0+4dN4g7BCd0Awe46MMU0ynnlaChMBw2EX2UG6Vtobi6y9uz7Gj7oS8T56NmXKpCP0eaJUFtuTCCK2mGT1j7Ur2KfVmcJWn7A93mRtS0A1EMyeI2NMk4xnXaFAPb2D6GwgFbKDdK3Gjfvg3fUtfrSnd5Ae2d7L/JtpEe+bZ3P83sll0hH6LFF69tzoRDhGErb7KEtM8vpHS8t0q5ltXPUJkwLXB1sCqoFg9hwZYxp1oC27DjAN5uhYCZ0NVKFq4OOD9MwZ05jtrVbjxn3w7vrkZY0P+nFMCxaflslVyqQr9OOiNKm+Va5tA+5GxZ37aeOHr7N2Xxd9wrTA9cGWgGogmD1H1phGxnPco/MijY6Vqn7jU2fDcpMaputL18CzBmmfBEqW+ODd9cnLyhv0I2yIeB/DspLKZEro865j4tom4PWPw5xcwqZw0SdMCNxKG18ql5nfqeVwNx+AYPYcnZP8Nm1jL4H60tmw3KSGjfoy5cHwUaBkhQ/eXZ+8rEmbRGt1JYKFqX7Eu44PfZTXP+a32Y1hFk0kNm7uMeKESDtZll0dqNVwN1+AYA4AVWPqw8AtgifWvrZ1H23fcxDe5hg2lud88IbmjbSeLFOrCL5MYnh2aGp9HX10zWIvygjcwesf61YttH7vpPCVtE6ItGNu0mpMBCaZ2VKXdQGAeaJk9dWf+9HZRJ6nyHD19A46LJHf2BC38+Y2Mj/3ZVIVIl2dbbShewkVW5uovq5AxdYm2tC9RGoAjgbw/qEzVCqXc9EPeHYIYrk24fWP5UuLzssickLokHbMFY2JqrYE2AMe5hzi07IsC5n0ZD7FW2eNjRUDn2Jd84SudzePm3x8t0PYR+EeX1Y/TDsh0rT1nt5Bqq8jKo1V/63Y2kT33XGDVpmAeSCYc4ovhomFTHqyPIUGpB2YbYhb38VMJT29g5MywMxpbsjdke95DZHx1Q7Z3kcBMe43NpwQOm09KXYZDgy/gGAGzqkUa/1DI8zv5CU0wMTAbEvc+ipmKmENKKEc+a4imnzfd5A3bHr0eX3+wMAp6jt0MrWIhhhPjy8rbLx2iDh/P4FgtgSMmpjKNHg2DJcv9Y9sFOkQbYbRFTcu2obqRMmXAdx3TL07mx59Xpvdubd/4r91PdrIMGSmDeg4IWzYDV47LJXLNfM+QwKC2QIwavLY8J76VP95XWp3hWgzjE4dumobqhOlkEJkssLku7Pp0U9Kp1eJ6qQvj7HuKphsAypOCFt2AytLYQHBbIE8GjWbXjnT3lOf6t8Xg+iLx10F0WYYIr06dNU2dCZKtbqKIIvJd2fToy+zqTmC1R5EfbXWJ+C8NvDI9hdp07Zea7bNlt3AylJYIK2cBfJm1EJLeeVT/fuQ4i+090d0scyjY+wTr4j06tBV20DaPvOYfHdpUgAmwevzLOLtIamv1nq74rWB0bGSVdtmy27YbIfAPPAwW8AXr6IpfPLYypBF/fO8QlkstcfLcvbcBeb3fH1/ROLY5TktDbTuRr0sGa7aRoieo93P9dNjO37h7aY00+/Olkef1ecXLZg1KYY5It4ekmyt6XYV2sqTrPfetG2zaTewshQOEMwWCHGwFOGTx1YG1/WfFN/m0iCyysLD1/dHxG9z9XUFeujjb9W+rqu2EVpMMq8Nb9l1QCmFn80Y8ZDsKqvPd7TPTGwPSbbWZLt69KlfGtmI6BKZlKRE5m1bSG0P2AOC2QKhDZZJhOYxt13/PntwZY9YJZJ7f1l5oGy1OZd9MyTPEa/dqKbws7kaFbpdlWkPMu3eRLvq6R1keryJ/F55ireBukKBRsdKVd8zPTaF3vZkCW3FwTUQzJYIabBMQmZ2XdnRFlzWTKuvn5/p89uqf10PrmlDxLueyg79JO9IltlGbHp08tQ3TZHUbmRFlO3VqLy/O1eeTNHE2ueVJ6LJbcBWWtKk+2bNhP1/+SzNu8SMsPUpu5SvQDCDRJJm1/GOdvDo6dx2NB0PrmlDJLoez0M1p7mBGqdPVfKOZBm7XiseHR42PD2iaybFhsqKKB9Wo0L2krlq96IJkq8rhyxq0U7YErah7VXKAghmIIVodu1DR3M1SOp4cE3Xj+h6PA+VzlHSWceuu/Lo+CawbAyISddMig2VFVFZx3rmwUvmot2LJkiid+VbXyHyy/PrAlvjbdb2PgQgmC3io3FJg2oYQJqOplJ3LgdJHQ+u6foRXc+kx8UHb6FtfBRYNgbEpGt2dbZRS8t02vzkC3Ri+LWq78kK3qw9fmnrLm82mwdvYrPq2qIzO5sUVlAr70IVW8K2Fux9WiCYLeHjQJwGnTAA3Y6mWncuPdw6HlzT9ZN0vSSPi+xAlLW30AU+rI7EsTEgylxz+dIiLS7OfL19pBW8ZSqXx//fJWnqLm82W4TOxMZkX0mq61p6F6rYEra1YO/TAsFsCR8H4jTohAHodjTVunO5lKQz0JiunzTXUxmIsvYWusDHZUgbA6LKNdMscWctdNLUXd5sdtLEWPU9m+wrSXWdt3dhElvCthbsfVogmC3hpr3+bwAAEiJJREFU40CcBpUwgPlt6bJkqNad66Uk1YHGtCFKcz3VgSjv8YE+LkPaGBCzzr7gSuikec482WwbExeTfSWprvP0LkxjU9jm3d6nBYLZEj4OxGlQCQNobW2moaFha/eKE8JSkmlDpHs9lYGoFmIIfWw7NgbErLMv2EyxWEma58yTzbYxcTHZV5LqOk/vwgaR/U871gI1IJgtoWpcfBcnLoWF6r2wlCSP7ECU9dI6Cxt9xNe2Y8PTk2X2BVspFlnoPqePkyddbHhoTfaVpLrO07sA+QGC2RIqxsVHcRLHpbDQuReWkuSQHYiyXlqPY7OPoO2YI6l9+dauKvF18qSDzZMyTS39E/Hr2ta78N0xBfwGgtkissbF50GkEpfCAiLGDrIDkW8xhKH0kVonqX351q7i5MXuuPDQphWfSWEFpt9FCI4p4DcQzB7g+yCSN2rdyyAzEPkWQ+hjH6n1dsRD1L58a1d5xba33Afxyet/vM8x6QZpgWD2AAwi7vDB0IeAbzGEvvURtCM9fGtXecamtzxr8cnrfwcGTtHOvf3Mz32cdIOwgGD2AAwi7sja0IeCb/GcvvURtCM9fIlNxepAOrIWn7z+t/v5I8zPd+7tpznNDcxTLOGYArJAMHuAb+Ikz2Rt6EPCp3hO3/oI2pE+Wcemul4dyKM4z3rFh9f/RsdKyteSPfApi3eYx7YTMhDMHlDdKSCWbZG1oZcFhrIanwR8KO2oFlD19rtcHchr6E7WKz68/je1vo4rmk+dOU8bupcoT7qzeod5bTshU5d1AWqdqFP0D52hUrk80Sl6egezLlouWbPsSs7n/oS/oE34TwjtqFZQ9fa7XB0QifOQ6epsow3dS6jY2kT1dQUqtjbRhu4lzoQcr/8tf8s87m8uv2QGdXW20X133ECbPruS7rvjhtRZrGyS17YTMvAwZwxiId3i29I+C7QJ/wmhHdUKqt5+l6sDvobumFjBynLFJ7rvll0HJuKS5zQ3UEf7TCKiSRv/InQns1m9Q1/bTi0DwZwx6BTu8WVpnzVorV3RjDYRCC7aEUJzklEND3AZTuBj6E6elvorN/GdGH6NvrZ1H23oXkId7TONTWazeoc+tp1aB4I5Y9Ap9AlZTPAGrZaW6VbbRMh1VmvkSdjEMdkOVb39LlcHso71ZZGXFSzRc8iGW8iQ1Tv0se3UOhDMGYNOoUfoYoJn7Lfs3G+tTYReZ7VGXoRNHBvtUNXbn3Z1IC7416++mhYXZzLvQ+RX6E5eVrBcPUdW79DHtlPrZCKYn3rqKfr+979Pf/M3f5PF7b0i5E6RpbcydDHBM/aHB4ettYnQ66zWyIuwiRN6O2QJ/ge/tZe76c2XELCIvKxqunyOrN6hb22n1nEumO+//3764Q9/SIsXL3Z9a28JsVOIvERrVzRbv3/oYoJn7Oe3jdedjTYRep3VWjhJXoRNnNDbYeiCPy+rmnl5DhAOztPKXXPNNXTvvfe6vi0wTNYpb+bNbWR+HoqY4KVFWrdqobV7hlxntZhqL6+p60Juh0ThC/6sU8KZIi/PAcLBmod5y5Yt9PWvf33SZ5///OfpXe96F/X09EhfZ/bsRpoypd508aRobbXvKQ2VIy+LBw3bdbd+9dX04Lf2Mj5f5Oy97X6un7bs3E+HBodpQVszrVu1kJYvLUr9du2KZmppmU5bdu6nw4PDNF/x9zr4UGe67HjmWc7nh2ntio5Jn/n+LLJk0UZc1F3I7ZCIaMFlzXTw6Omqz+e3NQdRfqLxthXvN1miW2++PUcWhNLmfES17grlcrlsqSxcenp66F//9V/pi1/8YuJ3h4aGHZSomtbW5szuHQIbN/cwl4uLrU30lbtXOam78SX6bGK/4yEpEWk9HLbbXZZ1loQo5OLOB3ZRiWGq6usKtOmzKyf+jX6rj8u687kdJmGr79cq6LP6oO704dWdSEQjSwbQwof4sSxjv0ONY/Q1Xj4pc0Je43lrFV/boQysTbnrVy9iZskAAOQHCGagRcjZPUwQehyjbyRNQHyYoJmk1jYw5o244IenD4D8k4lg7urqoq6urixuDQwSspcoLfB4miVpApKnCZrrfNgQ5wAAkB54mB2BQStf5M3j6QJRH5CZgJiYoPnQD12G8+CwGgCyQ9fe+GCnQDUQzAbhNXIMWvkjTx5PFyT1ARcTEF/6octwnlBj7WsVCKX8oGtvfLFToBoIZkOIGjkGrXxSyyEpqiT1ARcTEF/6octwHsTahwOEUr7QtTe+2ClQDQSzIUSNHIMWqHVk+oDtCYgv/dBlOA9i7cPBZ6EEz7c6uvbGFzsFqoFgNoSokWPQSgeMdfj40Ad8KAOR23AexNqHg45QcmEb4fnWQ9fe+GKnQDUQzIYQNfI1y67AoKUJjHU+8EG4yZTB1eTMVTgPYu3DQVUoubKNPnu+fUbX5vlgKwEbCGZDiBo5Bi19YKzzgQ99IKkMeZ2cIdY+DFSFkivbWEshAiYnzLo2zwdbCdhAMBsiqZHX+qCla4hqyVjnHR/6gKgMPk/OEJaUf1SFkivbKOP5zkP7tDFh1rV5PthKUA0Es0HQyNnIGiKW0UU8F3CFr5OzPHm+8yCsbKIyhriyjUme77y0T58nzMAP6rIuAMg/IkMUERnd/qEzVCqXJ4zuogWzmb9FPBcwzby5jczPs56cyfSfEOD18Z7ewayLFiRrll3J+dysbezqbKMN3Uuo2NpE9XUFKrY20YbuJRUe8YPM34XWPn2dMAN/gIe5hsjKuyNjiHhGt+/QK7ShewniuYB1fN1sk5eBHB48s7iMdRV5vvPSPrGaCZKAYK4Rslw2kzFEIqOLUBfgAl832+RlIM+LsPIJH2xjXtqnrxNm4A8QzDVClt4dGUOUF6PrirzFgvryPD4IkDh5GcjRx/NJXtqnrxNm4A8QzDVClt4dGUOUF6PrgrxssonI2/OYJi8DOfp4PslL+yTyc8IM/AGCuUbI2ruTZIjSGF1fvJOuyFssqI/P41ubysNAnidhBSaTh/bpEt/sC5ADgrlGCMG7o2N0a9E7mbdYUN+epxbblCsgrECtA/sSLkgrVyMkpQYKlbykNFLB1/Rnuvj2PLXYpgAAboB9CRd4mGuIPHp3fPNOuiCE1QIVfHueWmxTPb2DtOOZZ+nQr4exRAyARWrRvuQFCGYQNFnHZmdB3mJBfXueWmtTWCIGwB21Zl/yBAQzCBrfvJOuyNtqgU/PU2ttysdNl8As2GTmD7VmX/IEBDMIGt+8k1mCQdEMtdamsEScb7CC4Be1Zl/yBAQzCB6fvJNZgUHRLLXUprBEnG+SVhAw0XZPLdmXPIEsGQDkAOy8BrqsWXYl53MsEecB0QpCNNHuHzpDpXJ5YqLd0zvouJQA+A88zADkACyrA10iT9eOZw7T4cHhIJaI4RWVR7SCgPh1AOSBYM4AGHtgmrwuq6OvuKGrs43WruigoaHhrIuSCMKP1BBtMtu0rZf5G0y0AYtat8cQzI6BsQc2yOPOa/QVwAJeUTVEm8y27zmYm4l2rYs528AeQzA7B8Ye2CCPO6/RVwALhB+pw9tklpeJtqyYg6jWB/YYgtk5MPbAFnnbeY2+AljkNfwoC/Iy0ZYRc/CQpgP2GILZOTD2AMiBvgJY5MUr6gt5mGjLiDl4SNMBe4y0cs5BCicA5PClr/T0DtLGzT105wO7aOPmHqTcypiuzjba0L2Eiq1NVF9XoGJrE23oXgLRU8PMm9vI/LxSzMFDmg5f7HGWwMPsmLwsgQFgGx/6CpZx/SQPXlFgDplVB3hI0+GDPc4aCOYMgLEHQI6s+wqWcQHwHxkxh1Ce9GRtj7MGghkAADhgGRfklbxljEgSc/CQgrRAMAMAAAcs44I8UquhRrXuIQXpwKY/AADggI0uII+IQo0AAGzgYQYAAA5YxgV5xEaoUd5CPACIA8EMAAACsIwL8obpUKNaDfEAtQUEMwAAAOAJLjy1pjNGIJsMqAUgmAEAAAAPcOWpNR1qhGwyoBaAYAYAAOAMxLry4XlqH9n+IhGZF82mrodsMqAWQJYMAAAATog8qP1DZ6hULk94UHHc+Dg8T+3oWMnrekI2GVALQDADAABwAtKZiZk3t1H4d1/rqauzjTZ0L6FiaxPV1xWo2NpEG7qXYOUA5AqEZAAAAHACYl3F8DbjRfhcT8gmA/IOPMwAAACcwPOgItZ1nMhTO7WePTSjngDIDghmAAAATkCsazJdnW300TWLmX9DPQGQHQjJAKDGQdYC4AqcnCgH6gkA/4BgBqCGwQldwDWIdZUD9QSAXyAkA4AaBlkLAAAAgGQgmAGoYZC1AAAAAEgGghmAGgZZCwAAAIBkIJgBqGGQtQAAAABIBpv+AKhhsBu/9kBWFAAAUAeCGYAaB7vxawdRVpS1K5qzKhYAAHgPQjIAAKBGQFYUAADQA4IZAABqBGRFAQAAPSCYAQCgRkBWFAAA0AOCGQAAagRkRQEAAD2w6Q8AAGoEZEUBAAA9IJgBAKCGQFYUAABQByEZAAAAAAAACIBgBgAAAAAAQAAEMwAAAAAAAAIgmAEAAAAAABAAwQwAAAAAAIAACOb/v737Z4krCwM4/GYTElBTqYiyoLVFIE10qnyAgd2vICypAwmGkGKLBdMs1iHFElJbuAjzGYb5ABECISiIIqOVGYskZraK7EbnNetw7+Q6z9Pdg39eTiE/5NwzAACQEMwAAJAQzAAAkPDBJXCB1uZ+NJpbsXtwHDMTI1GvzfngBwAYIoIZEq3N/Xi58eb0eafdOX0WzQAwHBzJgESjudVjfbvUOQCAwRHMkNg9OD53fe+wU/IkAMCgCGZIzEyMnLs+PT5a8iQAwKAIZkjUa3M91mfLHQQAGBgv/UHi64t9jeZ27B12Ynp8NOq1WS/8AcAQEcxwgYX5KYEMAEPMkQwAAEgIZgAASAhmAABICGYAAEgIZgAASAhmAABICGYAAEiUeg/z0dFRLC8vx4cPH+LTp0/x9OnTuHv3bpkjAABwgdbmfjSaW7F7cBwzEyNRr80N9WcSlBrMr169isXFxVhaWor379/H48ePY319vcwRAABItDb34+XGm9PnnXbn9HlYo7nUYF5aWoqbN29GRMTJyUncunWrzF8PAMAFGs2tHuvbQxvM17rdbreIH7y2thavX7/+z9rz58/jzp070W6348GDB/Hs2bO4d+9e+nM+fz6JGzeuFzEiAADf+HV5I758OZuH13+6Fn//+csAJhq8woK5l7dv38ajR4/iyZMncf/+/Qu/vt0+KmGqsyYnbw/sd1edvbs8e9cf+3d59u7y7N3l2bvLK3Lvfv+rFTvtzpn1nyfH4o/f8n90VkGvvZucvN3ze0q9JePdu3fx8OHDWF1d/a5YBgCgXPXaXI/12XIH+YGUeoZ5dXU1Pn78GCsrKxERMTY2Fi9evChzBAAAEl/PKTea27F32Inp8dGo12aH9vxyRMnBLI4BgEFxVdr3W5ifsjf/UmowAwAMgqvS6IdP+gMArrzsqjS4iGAGAK683YPjc9f3Ds/eBgHfEswAwJU3MzFy7vr0+GjJk1BFghkAuPJclUY/vPQHAFx5rkqjH4IZABgKrkrjshzJAACAhGAGAICEYAYAgIRgBgCAhGAGAICEYAYAgIRgBgCAhGAGAICEYAYAgIRgBgCAhGAGAICEYAYAgIRgBgCAhGAGAICEYAYAgIRgBgCAhGAGAICEYAYAgIRgBgCAhGAGAICEYAYAgMSNQQ8AAHCVtTb3o9Hcit2D45iZGIl6bS4W5qcGPRb/g2AGAChIa3M/Xm68OX3eaXdOn0VzdTiSAQBQkEZzq8f6dqlz0B/BDABQkN2D43PX9w47JU9CPwQzAEBBZiZGzl2fHh8teRL6IZgBAApSr831WJ8tdxD64qU/AICCfH2xr9Hcjr3DTkyPj0a9NuuFv4oRzEApXKsEDKuF+Sl/7ypOMAOFc60SAFXmDDNQONcqAVBlghkonGuVAKgywQwUzrVKAFSZYAYK51olAKrMS39A4VyrBECVCWagFK5VAqCqHMkAAICEYAYAgIRgBgCAhGAGAICEYAYAgIRgBgCAhGAGAICEYAYAgIRgBgCAhGAGAICEYAYAgIRgBgCAhGAGAICEYAYAgIRgBgCAxLVut9sd9BAAAPCj8h9mAABICGYAAEgIZgAASAhmAABICGYAAEgIZgAASPwDW+bsy09zVYwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fit1.resid.plot(style='o', figsize=(12,8))\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Risidual Plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diagnostic_plots.diagnostic_plots(dfs_clean.drop('Rating', axis=1), dfs_clean['Rating'], fit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Rating</td>      <th>  R-squared:         </th> <td>   0.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.445</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   26.71</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 20 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>4.49e-84</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:56:03</td>     <th>  Log-Likelihood:    </th> <td>  357.45</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   770</td>      <th>  AIC:               </th> <td>  -664.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   745</td>      <th>  BIC:               </th> <td>  -548.7</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    24</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                       <td>    0.0571</td> <td>    0.015</td> <td>    3.875</td> <td> 0.000</td> <td>    0.028</td> <td>    0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Level of difficulty</th>         <td>   -0.0717</td> <td>    0.009</td> <td>   -7.922</td> <td> 0.000</td> <td>   -0.089</td> <td>   -0.054</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Total reviews</th>               <td>    0.0031</td> <td>    0.008</td> <td>    0.401</td> <td> 0.688</td> <td>   -0.012</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Student size</th>                <td> 3.152e-05</td> <td> 8.06e-06</td> <td>    3.913</td> <td> 0.000</td> <td> 1.57e-05</td> <td> 4.73e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Type_public</th>                 <td>    0.1439</td> <td>    0.036</td> <td>    4.006</td> <td> 0.000</td> <td>    0.073</td> <td>    0.214</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Type_private</th>                <td>   -0.0868</td> <td>    0.021</td> <td>   -4.076</td> <td> 0.000</td> <td>   -0.129</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Region_west</th>                 <td>    0.2302</td> <td>    0.060</td> <td>    3.817</td> <td> 0.000</td> <td>    0.112</td> <td>    0.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Region_east</th>                 <td>   -0.0868</td> <td>    0.021</td> <td>   -4.076</td> <td> 0.000</td> <td>   -0.129</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Region_south</th>                <td>   -0.0864</td> <td>    0.025</td> <td>   -3.436</td> <td> 0.001</td> <td>   -0.136</td> <td>   -0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Accessible outside class</th>    <td>   -0.2086</td> <td>    0.456</td> <td>   -0.458</td> <td> 0.647</td> <td>   -1.104</td> <td>    0.686</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amazing lectures</th>            <td>   -0.1600</td> <td>    0.452</td> <td>   -0.354</td> <td> 0.723</td> <td>   -1.048</td> <td>    0.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Beware of pop quizzes</th>       <td>   -0.2829</td> <td>    0.473</td> <td>   -0.598</td> <td> 0.550</td> <td>   -1.211</td> <td>    0.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Caring</th>                      <td>   -0.1962</td> <td>    0.452</td> <td>   -0.434</td> <td> 0.664</td> <td>   -1.084</td> <td>    0.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Clear grading criteria</th>      <td>   -0.4836</td> <td>    0.453</td> <td>   -1.067</td> <td> 0.286</td> <td>   -1.373</td> <td>    0.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Extra credit</th>                <td>   -0.3795</td> <td>    0.458</td> <td>   -0.829</td> <td> 0.407</td> <td>   -1.278</td> <td>    0.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Get ready to read</th>           <td>   -0.4835</td> <td>    0.452</td> <td>   -1.070</td> <td> 0.285</td> <td>   -1.370</td> <td>    0.403</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Gives good feedback</th>         <td>   -0.3227</td> <td>    0.451</td> <td>   -0.716</td> <td> 0.474</td> <td>   -1.207</td> <td>    0.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Graded by few things</th>        <td>   -0.8318</td> <td>    0.459</td> <td>   -1.812</td> <td> 0.070</td> <td>   -1.733</td> <td>    0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group projects</th>              <td>   -0.8294</td> <td>    0.461</td> <td>   -1.799</td> <td> 0.072</td> <td>   -1.734</td> <td>    0.076</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Hilarious</th>                   <td>   -0.3107</td> <td>    0.452</td> <td>   -0.688</td> <td> 0.492</td> <td>   -1.198</td> <td>    0.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Inspirational</th>               <td>   -0.3438</td> <td>    0.451</td> <td>   -0.762</td> <td> 0.446</td> <td>   -1.230</td> <td>    0.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lecture heavy</th>               <td>   -0.6921</td> <td>    0.453</td> <td>   -1.529</td> <td> 0.127</td> <td>   -1.581</td> <td>    0.196</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lots of homework</th>            <td>   -0.5434</td> <td>    0.451</td> <td>   -1.206</td> <td> 0.228</td> <td>   -1.428</td> <td>    0.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Participation matters</th>       <td>   -0.4245</td> <td>    0.452</td> <td>   -0.940</td> <td> 0.348</td> <td>   -1.312</td> <td>    0.462</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Respected</th>                   <td>   -0.2707</td> <td>    0.451</td> <td>   -0.600</td> <td> 0.549</td> <td>   -1.157</td> <td>    0.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skip class? you won't pass.</th> <td>   -0.3854</td> <td>    0.451</td> <td>   -0.854</td> <td> 0.394</td> <td>   -1.271</td> <td>    0.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>So many papers</th>              <td>   -0.7653</td> <td>    0.460</td> <td>   -1.664</td> <td> 0.097</td> <td>   -1.668</td> <td>    0.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Test heavy</th>                  <td>   -0.9672</td> <td>    0.462</td> <td>   -2.095</td> <td> 0.036</td> <td>   -1.873</td> <td>   -0.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Tough grader</th>                <td>   -0.6692</td> <td>    0.450</td> <td>   -1.486</td> <td> 0.138</td> <td>   -1.553</td> <td>    0.215</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>159.954</td> <th>  Durbin-Watson:     </th> <td>   2.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 386.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-1.091</td>  <th>  Prob(JB):          </th> <td>1.17e-84</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.700</td>  <th>  Cond. No.          </th> <td>3.11e+17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.22e-23. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 Rating   R-squared:                       0.463\n",
       "Model:                            OLS   Adj. R-squared:                  0.445\n",
       "Method:                 Least Squares   F-statistic:                     26.71\n",
       "Date:                Fri, 20 Jul 2018   Prob (F-statistic):           4.49e-84\n",
       "Time:                        01:56:03   Log-Likelihood:                 357.45\n",
       "No. Observations:                 770   AIC:                            -664.9\n",
       "Df Residuals:                     745   BIC:                            -548.7\n",
       "Df Model:                          24                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================================\n",
       "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------\n",
       "const                           0.0571      0.015      3.875      0.000       0.028       0.086\n",
       "Level of difficulty            -0.0717      0.009     -7.922      0.000      -0.089      -0.054\n",
       "Total reviews                   0.0031      0.008      0.401      0.688      -0.012       0.018\n",
       "Student size                 3.152e-05   8.06e-06      3.913      0.000    1.57e-05    4.73e-05\n",
       "Type_public                     0.1439      0.036      4.006      0.000       0.073       0.214\n",
       "Type_private                   -0.0868      0.021     -4.076      0.000      -0.129      -0.045\n",
       "Region_west                     0.2302      0.060      3.817      0.000       0.112       0.349\n",
       "Region_east                    -0.0868      0.021     -4.076      0.000      -0.129      -0.045\n",
       "Region_south                   -0.0864      0.025     -3.436      0.001      -0.136      -0.037\n",
       "Accessible outside class       -0.2086      0.456     -0.458      0.647      -1.104       0.686\n",
       "Amazing lectures               -0.1600      0.452     -0.354      0.723      -1.048       0.728\n",
       "Beware of pop quizzes          -0.2829      0.473     -0.598      0.550      -1.211       0.645\n",
       "Caring                         -0.1962      0.452     -0.434      0.664      -1.084       0.691\n",
       "Clear grading criteria         -0.4836      0.453     -1.067      0.286      -1.373       0.406\n",
       "Extra credit                   -0.3795      0.458     -0.829      0.407      -1.278       0.519\n",
       "Get ready to read              -0.4835      0.452     -1.070      0.285      -1.370       0.403\n",
       "Gives good feedback            -0.3227      0.451     -0.716      0.474      -1.207       0.562\n",
       "Graded by few things           -0.8318      0.459     -1.812      0.070      -1.733       0.069\n",
       "Group projects                 -0.8294      0.461     -1.799      0.072      -1.734       0.076\n",
       "Hilarious                      -0.3107      0.452     -0.688      0.492      -1.198       0.576\n",
       "Inspirational                  -0.3438      0.451     -0.762      0.446      -1.230       0.542\n",
       "Lecture heavy                  -0.6921      0.453     -1.529      0.127      -1.581       0.196\n",
       "Lots of homework               -0.5434      0.451     -1.206      0.228      -1.428       0.341\n",
       "Participation matters          -0.4245      0.452     -0.940      0.348      -1.312       0.462\n",
       "Respected                      -0.2707      0.451     -0.600      0.549      -1.157       0.615\n",
       "Skip class? you won't pass.    -0.3854      0.451     -0.854      0.394      -1.271       0.501\n",
       "So many papers                 -0.7653      0.460     -1.664      0.097      -1.668       0.138\n",
       "Test heavy                     -0.9672      0.462     -2.095      0.036      -1.873      -0.061\n",
       "Tough grader                   -0.6692      0.450     -1.486      0.138      -1.553       0.215\n",
       "==============================================================================\n",
       "Omnibus:                      159.954   Durbin-Watson:                   2.025\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              386.518\n",
       "Skew:                          -1.091   Prob(JB):                     1.17e-84\n",
       "Kurtosis:                       5.700   Cond. No.                     3.11e+17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.22e-23. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tested Box transformation and noted worse result\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "dfs_box=dfs_clean.copy()\n",
    "lamb=stats.boxcox_normmax(dfs_box.Rating, brack=(-1.9, 1.9))\n",
    "\n",
    "dfs_box.Rating=(np.power(dfs_box.Rating,-0.2282)-1)/-0.2282\n",
    "\n",
    "plt.hist(dfs_box.Rating);\n",
    "\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2=tt_split(dfs_box, 'Rating', 0.8)\n",
    "ssX2 = StandardScaler()\n",
    "X_train_scaled2 = ssX.fit_transform(X_train2)\n",
    "\n",
    "\n",
    "\n",
    "model2=sm.OLS(y_train2, sm.add_constant(X_train2))\n",
    "fit2=model2.fit()\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Rating</td>      <th>  R-squared:         </th> <td>   0.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   27.61</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 20 Jul 2018</td> <th>  Prob (F-statistic):</th> <td>1.68e-86</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:56:05</td>     <th>  Log-Likelihood:    </th> <td>  169.04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   770</td>      <th>  AIC:               </th> <td>  -288.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   745</td>      <th>  BIC:               </th> <td>  -171.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    24</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                  <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                       <td>    0.0666</td> <td>    0.019</td> <td>    3.538</td> <td> 0.000</td> <td>    0.030</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Level of difficulty</th>         <td>   -0.0925</td> <td>    0.012</td> <td>   -8.001</td> <td> 0.000</td> <td>   -0.115</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Total reviews</th>               <td>    0.0014</td> <td>    0.010</td> <td>    0.139</td> <td> 0.890</td> <td>   -0.018</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Student size</th>                <td> 3.692e-05</td> <td> 1.03e-05</td> <td>    3.588</td> <td> 0.000</td> <td> 1.67e-05</td> <td> 5.71e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Type_public</th>                 <td>    0.1686</td> <td>    0.046</td> <td>    3.676</td> <td> 0.000</td> <td>    0.079</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Type_private</th>                <td>   -0.1021</td> <td>    0.027</td> <td>   -3.753</td> <td> 0.000</td> <td>   -0.155</td> <td>   -0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Region_west</th>                 <td>    0.2679</td> <td>    0.077</td> <td>    3.478</td> <td> 0.001</td> <td>    0.117</td> <td>    0.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Region_east</th>                 <td>   -0.1021</td> <td>    0.027</td> <td>   -3.753</td> <td> 0.000</td> <td>   -0.155</td> <td>   -0.049</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Region_south</th>                <td>   -0.0993</td> <td>    0.032</td> <td>   -3.093</td> <td> 0.002</td> <td>   -0.162</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Accessible outside class</th>    <td>   -0.2056</td> <td>    0.582</td> <td>   -0.353</td> <td> 0.724</td> <td>   -1.349</td> <td>    0.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Amazing lectures</th>            <td>   -0.1285</td> <td>    0.577</td> <td>   -0.223</td> <td> 0.824</td> <td>   -1.262</td> <td>    1.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Beware of pop quizzes</th>       <td>   -0.3050</td> <td>    0.604</td> <td>   -0.505</td> <td> 0.614</td> <td>   -1.490</td> <td>    0.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Caring</th>                      <td>   -0.1724</td> <td>    0.577</td> <td>   -0.299</td> <td> 0.765</td> <td>   -1.306</td> <td>    0.961</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Clear grading criteria</th>      <td>   -0.5594</td> <td>    0.579</td> <td>   -0.967</td> <td> 0.334</td> <td>   -1.696</td> <td>    0.577</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Extra credit</th>                <td>   -0.4222</td> <td>    0.585</td> <td>   -0.722</td> <td> 0.470</td> <td>   -1.570</td> <td>    0.726</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Get ready to read</th>           <td>   -0.5568</td> <td>    0.577</td> <td>   -0.965</td> <td> 0.335</td> <td>   -1.689</td> <td>    0.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Gives good feedback</th>         <td>   -0.3386</td> <td>    0.576</td> <td>   -0.588</td> <td> 0.557</td> <td>   -1.469</td> <td>    0.791</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Graded by few things</th>        <td>   -0.9900</td> <td>    0.586</td> <td>   -1.689</td> <td> 0.092</td> <td>   -2.141</td> <td>    0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Group projects</th>              <td>   -1.0072</td> <td>    0.589</td> <td>   -1.711</td> <td> 0.088</td> <td>   -2.163</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Hilarious</th>                   <td>   -0.3246</td> <td>    0.577</td> <td>   -0.562</td> <td> 0.574</td> <td>   -1.458</td> <td>    0.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Inspirational</th>               <td>   -0.3705</td> <td>    0.576</td> <td>   -0.643</td> <td> 0.521</td> <td>   -1.502</td> <td>    0.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lecture heavy</th>               <td>   -0.8213</td> <td>    0.578</td> <td>   -1.421</td> <td> 0.156</td> <td>   -1.956</td> <td>    0.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lots of homework</th>            <td>   -0.6307</td> <td>    0.575</td> <td>   -1.096</td> <td> 0.273</td> <td>   -1.760</td> <td>    0.499</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Participation matters</th>       <td>   -0.4814</td> <td>    0.577</td> <td>   -0.834</td> <td> 0.404</td> <td>   -1.614</td> <td>    0.652</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Respected</th>                   <td>   -0.2723</td> <td>    0.576</td> <td>   -0.472</td> <td> 0.637</td> <td>   -1.404</td> <td>    0.859</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skip class? you won't pass.</th> <td>   -0.4330</td> <td>    0.577</td> <td>   -0.751</td> <td> 0.453</td> <td>   -1.565</td> <td>    0.699</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>So many papers</th>              <td>   -0.8835</td> <td>    0.588</td> <td>   -1.504</td> <td> 0.133</td> <td>   -2.037</td> <td>    0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Test heavy</th>                  <td>   -1.1780</td> <td>    0.590</td> <td>   -1.998</td> <td> 0.046</td> <td>   -2.335</td> <td>   -0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Tough grader</th>                <td>   -0.7952</td> <td>    0.575</td> <td>   -1.382</td> <td> 0.167</td> <td>   -1.925</td> <td>    0.334</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>122.185</td> <th>  Durbin-Watson:     </th> <td>   2.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 233.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.935</td>  <th>  Prob(JB):          </th> <td>2.15e-51</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.944</td>  <th>  Cond. No.          </th> <td>3.11e+17</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.22e-23. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 Rating   R-squared:                       0.471\n",
       "Model:                            OLS   Adj. R-squared:                  0.454\n",
       "Method:                 Least Squares   F-statistic:                     27.61\n",
       "Date:                Fri, 20 Jul 2018   Prob (F-statistic):           1.68e-86\n",
       "Time:                        01:56:05   Log-Likelihood:                 169.04\n",
       "No. Observations:                 770   AIC:                            -288.1\n",
       "Df Residuals:                     745   BIC:                            -171.9\n",
       "Df Model:                          24                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================================\n",
       "                                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------------\n",
       "const                           0.0666      0.019      3.538      0.000       0.030       0.103\n",
       "Level of difficulty            -0.0925      0.012     -8.001      0.000      -0.115      -0.070\n",
       "Total reviews                   0.0014      0.010      0.139      0.890      -0.018       0.021\n",
       "Student size                 3.692e-05   1.03e-05      3.588      0.000    1.67e-05    5.71e-05\n",
       "Type_public                     0.1686      0.046      3.676      0.000       0.079       0.259\n",
       "Type_private                   -0.1021      0.027     -3.753      0.000      -0.155      -0.049\n",
       "Region_west                     0.2679      0.077      3.478      0.001       0.117       0.419\n",
       "Region_east                    -0.1021      0.027     -3.753      0.000      -0.155      -0.049\n",
       "Region_south                   -0.0993      0.032     -3.093      0.002      -0.162      -0.036\n",
       "Accessible outside class       -0.2056      0.582     -0.353      0.724      -1.349       0.938\n",
       "Amazing lectures               -0.1285      0.577     -0.223      0.824      -1.262       1.005\n",
       "Beware of pop quizzes          -0.3050      0.604     -0.505      0.614      -1.490       0.880\n",
       "Caring                         -0.1724      0.577     -0.299      0.765      -1.306       0.961\n",
       "Clear grading criteria         -0.5594      0.579     -0.967      0.334      -1.696       0.577\n",
       "Extra credit                   -0.4222      0.585     -0.722      0.470      -1.570       0.726\n",
       "Get ready to read              -0.5568      0.577     -0.965      0.335      -1.689       0.576\n",
       "Gives good feedback            -0.3386      0.576     -0.588      0.557      -1.469       0.791\n",
       "Graded by few things           -0.9900      0.586     -1.689      0.092      -2.141       0.161\n",
       "Group projects                 -1.0072      0.589     -1.711      0.088      -2.163       0.149\n",
       "Hilarious                      -0.3246      0.577     -0.562      0.574      -1.458       0.808\n",
       "Inspirational                  -0.3705      0.576     -0.643      0.521      -1.502       0.761\n",
       "Lecture heavy                  -0.8213      0.578     -1.421      0.156      -1.956       0.314\n",
       "Lots of homework               -0.6307      0.575     -1.096      0.273      -1.760       0.499\n",
       "Participation matters          -0.4814      0.577     -0.834      0.404      -1.614       0.652\n",
       "Respected                      -0.2723      0.576     -0.472      0.637      -1.404       0.859\n",
       "Skip class? you won't pass.    -0.4330      0.577     -0.751      0.453      -1.565       0.699\n",
       "So many papers                 -0.8835      0.588     -1.504      0.133      -2.037       0.270\n",
       "Test heavy                     -1.1780      0.590     -1.998      0.046      -2.335      -0.021\n",
       "Tough grader                   -0.7952      0.575     -1.382      0.167      -1.925       0.334\n",
       "==============================================================================\n",
       "Omnibus:                      122.185   Durbin-Watson:                   2.029\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              233.336\n",
       "Skew:                          -0.935   Prob(JB):                     2.15e-51\n",
       "Kurtosis:                       4.944   Cond. No.                     3.11e+17\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.22e-23. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tested log transform of y, and noted worse result\n",
    "\n",
    "dfs_log=dfs_clean.copy()\n",
    "dfs_log['Rating']=np.log(dfs_log['Rating'])\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3=tt_split(dfs_log, 'Rating', 0.8)\n",
    "ssX3 = StandardScaler()\n",
    "X_train_scaled3 = ssX.fit_transform(X_train3)\n",
    "\n",
    "\n",
    "model3=sm.OLS(y_train3, sm.add_constant(X_train3))\n",
    "fit3=model3.fit()\n",
    "fit3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Rating', 'Level of difficulty', 'Total reviews', 'Student size',\n",
       "       'Type_public', 'Type_private', 'Region_west', 'Region_east',\n",
       "       'Region_south', 'Accessible outside class', 'Amazing lectures',\n",
       "       'Beware of pop quizzes', 'Caring', 'Clear grading criteria',\n",
       "       'Extra credit', 'Get ready to read', 'Gives good feedback',\n",
       "       'Graded by few things', 'Group projects', 'Hilarious', 'Inspirational',\n",
       "       'Lecture heavy', 'Lots of homework', 'Participation matters',\n",
       "       'Respected', 'Skip class? you won't pass.', 'So many papers',\n",
       "       'Test heavy', 'Tough grader'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran iterations of models and only keep key features that can impact rating\n",
    "\n",
    "dfs_clean_smaller=dfs_clean.loc[:,['Rating', 'Level of difficulty',  \n",
    "        'Accessible outside class', 'Amazing lectures','Caring', 'Clear grading criteria',\n",
    "        'Gives good feedback',\n",
    "        'Hilarious', 'Inspirational',\n",
    "         'Participation matters',\n",
    "       'Respected', 'Skip class? you won\\'t pass.' \n",
    "        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 4 with lasso approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=tt_split(dfs_clean_smaller, 'Rating', 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSX4=StandardScaler()\n",
    "X_train_scaled=SSX4.fit_transform(X_train)\n",
    "X_test_scaled=SSX4.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline\n",
    "\n",
    "est=make_pipeline(PolynomialFeatures(2), Lasso(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'polynomialfeatures', 'lasso', 'polynomialfeatures__degree', 'polynomialfeatures__include_bias', 'polynomialfeatures__interaction_only', 'lasso__alpha', 'lasso__copy_X', 'lasso__fit_intercept', 'lasso__max_iter', 'lasso__normalize', 'lasso__positive', 'lasso__precompute', 'lasso__random_state', 'lasso__selection', 'lasso__tol', 'lasso__warm_start'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#est.get_params().keys()\n",
    "#est.set_params(polynomialfeatures__degree=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/Users/xzhou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('lasso', Lasso(alpha=1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'lasso__alpha': [0.0001, 0.001, 0.01, 0.1, 1], 'polynomialfeatures__degree': [1, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'lasso__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1] , 'polynomialfeatures__degree': [1, 2, 3]}\n",
    "grid = GridSearchCV(est, param_grid=params, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.24176278,  0.09592238,  0.22397237,  0.20306984,\n",
       "        0.04795784,  0.16809506,  0.13677695,  0.1489329 ,  0.08689825,\n",
       "        0.18263077,  0.08757893])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.get_params()['lasso'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lasso__alpha': 0.001, 'polynomialfeatures__degree': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3802646865722208"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Level of difficulty</th>\n",
       "      <th>Accessible outside class</th>\n",
       "      <th>Amazing lectures</th>\n",
       "      <th>Caring</th>\n",
       "      <th>Clear grading criteria</th>\n",
       "      <th>Gives good feedback</th>\n",
       "      <th>Hilarious</th>\n",
       "      <th>Inspirational</th>\n",
       "      <th>Participation matters</th>\n",
       "      <th>Respected</th>\n",
       "      <th>Skip class? you won't pass.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>nyu</th>\n",
       "      <th>1291</th>\n",
       "      <td>3.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uf</th>\n",
       "      <th>1855</th>\n",
       "      <td>3.6</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>berkely</th>\n",
       "      <th>2244</th>\n",
       "      <td>2.7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">uf</th>\n",
       "      <th>4249</th>\n",
       "      <td>2.8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>3.1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Level of difficulty  Accessible outside class  Amazing lectures  \\\n",
       "nyu     1291                  3.2                      0.00              0.09   \n",
       "uf      1855                  3.6                      0.08              0.08   \n",
       "berkely 2244                  2.7                      0.00              0.33   \n",
       "uf      4249                  2.8                      0.00              0.14   \n",
       "        1437                  3.1                      0.08              0.14   \n",
       "\n",
       "              Caring  Clear grading criteria  Gives good feedback  Hilarious  \\\n",
       "nyu     1291    0.18                    0.00                 0.09       0.00   \n",
       "uf      1855    0.00                    0.00                 0.00       0.08   \n",
       "berkely 2244    0.00                    0.00                 0.00       0.33   \n",
       "uf      4249    0.14                    0.10                 0.00       0.05   \n",
       "        1437    0.08                    0.02                 0.06       0.18   \n",
       "\n",
       "              Inspirational  Participation matters  Respected  \\\n",
       "nyu     1291           0.18                    0.0       0.18   \n",
       "uf      1855           0.00                    0.0       0.00   \n",
       "berkely 2244           0.00                    0.0       0.00   \n",
       "uf      4249           0.05                    0.0       0.10   \n",
       "        1437           0.04                    0.0       0.04   \n",
       "\n",
       "              Skip class? you won't pass.  \n",
       "nyu     1291                         0.00  \n",
       "uf      1855                         0.00  \n",
       "berkely 2244                         0.00  \n",
       "uf      4249                         0.00  \n",
       "        1437                         0.04  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modle 5 with Ridge model\n",
    "\n",
    "degree=2\n",
    "alphas= 1  #[1e-5, 1e-4, 1e-3,1e-2,1e-1]\n",
    "\n",
    "\n",
    "est2=make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est2.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params2 = {'ridge__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1] , 'polynomialfeatures__degree': [1, 2, 3]}\n",
    "grid2 = GridSearchCV(est2, param_grid=params2, cv=5)\n",
    "grid2.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2.best_estimator_.get_params()['ridge'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid2.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 6: Elastic Net\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=2\n",
    "alphas=1\n",
    "\n",
    "est3=make_pipeline(PolynomialFeatures(degree), ElasticNet(alpha=alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est3.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {'elasticnet__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1] , 'polynomialfeatures__degree': [1, 2, 3]}\n",
    "grid3 = GridSearchCV(est3, param_grid=params, cv=5)\n",
    "grid3.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid3.best_estimator_.get_params()['elasticnet'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid3.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit4.resid.plot(style='o', figsize=(12,8));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots.diagnostic_plots(dfs_clean.drop(['Name','Rating'], axis=1), dfs_clean['Rating'], fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
